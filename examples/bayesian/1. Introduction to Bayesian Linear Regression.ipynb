{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This series of notebooks offers an in-depth exploration of the **Bayesian Linear Regression**. \n",
    "It includes a comparison with the frequentist approach to linear regression and an introduction to the most common Bayesian inference techniques.\n",
    "\n",
    "The notebooks are organized as follows:\n",
    "\n",
    "1. **Foundations of Linear Regression**  \n",
    "   The first notebook (this one) lays the groundwork for understanding linear regression. It introduces the mathematical framework and provides an overview of frequentist approaches to estimate the model weights. This serves as a stepping stone for transitioning into the Bayesian perspective.\n",
    "\n",
    "\n",
    "2. **Conjugate Prior in Bayesian Inference**  \n",
    "   The second notebook delves into the concept of conjugate priors. By using conjugate distributions, we can derive posterior distributions analytically, simplifying Bayesian inference. This notebook also highlights how prior knowledge can influence the model and improve predictions in the presence of limited data.\n",
    "\n",
    "\n",
    "3. **MCMC and Variational Inference**  \n",
    "   The third and fourth notebooks introduce advanced methods for Bayesian inference, including Markov Chain Monte Carlo (MCMC) and Variational Inference. These techniques enable approximate inference when analytical solutions are infeasible, making it possible to handle complex models and large datasets effectively.\n",
    "\n",
    "These notebooks are heavily inspired by the theories and notations presented in Christopher M. Bishop's classic textbook *[Pattern Recognition and Machine Learning](https://www.microsoft.com/en-us/research/people/cmbishop/prml-book/)*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Helper Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import Math, display\n",
    "from sklearn.model_selection import train_test_split\n",
    "from utils import style_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAxBG7mR2Ar1"
   },
   "source": [
    "# Theory of Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression is a widely used model due to its simplicity and interpretability. \n",
    "\n",
    "\n",
    "In its simplest form, it predicts a single target $t$ as the deterministic output of the function $y$, which in turn is a linear combination of input variables $\\mathbf{x} = (x_1, \\dots, x_D)^\\top$ and parameters $\\mathbf{w} = (w_0, w_1, \\dots, w_D)^\\top$:\n",
    "\n",
    "$$\n",
    "t = y(\\mathbf{x}, \\mathbf{w}) = w_0 + \\sum_{j=1}^D w_j x_j \\tag{1}\n",
    "$$\n",
    "\n",
    "\n",
    "The parameter $w_0$, which is often called \"bias\" or \"intercept\", allows for any fixed offset in the data. It is often convenient to define an additional dummy feature $x_0 = 1$ so that we can simplify the above equation as:\n",
    "\n",
    "$$\n",
    "t =  y(\\mathbf{x}, \\mathbf{w}) = \\mathbf{w}^\\top \\mathbf{x} \\tag{2}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood of a single target data point $t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Likelihood measures how well a statistical model explains the observed data, given its parameters.** Put another way, it expresses how the observed data could have been generated by the model’s assumed data-generating process.\n",
    "\n",
    "Let’s revisit our earlier example. Initially, we assumed that the target $t$ is a deterministic output of $y(\\mathbf{x}, \\mathbf{w})$, the model's prediction based on the input features $\\mathbf{x}$ and the regression coefficients $\\mathbf{w}$. \n",
    "\n",
    "While this works in theory, it doesn’t account for the inherent uncertainty in real-world data.\n",
    "To address this, we shall introduce uncertainty by assuming that $t$ is generated as the deterministic output $y(\\mathbf{x}, \\mathbf{w})$ combined with additive Gaussian noise $\\epsilon$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "t &= y(\\mathbf{x}, \\mathbf{w}) + \\epsilon \\\\\n",
    "&= \\mathbf{w}^\\top \\mathbf{x} \\tag{3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "where $\\epsilon \\sim \\mathcal{N}(0, \\beta^{-1})$ represents noise with zero mean and variance $\\beta^{-1}$ (i.e. in $\\beta$ is the *precision* of the Gaussian).\n",
    "\n",
    "\n",
    "Alternatively, we can express  $t$ probabilistically by saying that it follows a Gaussian distribution, centered at $y(\\mathbf{x}, \\mathbf{w})$, with variance $\\beta^{-1}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(t | \\mathbf{x}, \\mathbf{w}, \\beta) &= \\mathcal{N}(t | y(\\mathbf{x}, \\mathbf{w}), \\beta^{-1}) \\\\\n",
    "&= \\mathcal{N}(t | \\mathbf{w}^\\top \\mathbf{x}, \\beta^{-1}) \\tag{4}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood of a set of data points $\\mathbf{t}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now let's consider a data set of inputs $ \\mathbf{X} = \\{ \\mathbf{x}_1, \\ldots, \\mathbf{x}_N \\} $ with corresponding target values $ t_1, \\ldots, t_N $. \n",
    "\n",
    "First, we group the scalar target variables $\\{ t_n \\}$ into a column vector that we denote by $\\mathbf{t}$. \n",
    "\n",
    "Afterards, we assume that the data points in $\\mathbf{t}$ are drawn independently from the above single-sample likelihood distribution. With this assumption, we proceed to construct the following expression for the likelihood function of the *entire dataset* $\\mathbf{t}$:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\beta) &= \\prod_{n=1}^N \\mathcal{N}(t_n | \\mathbf{w}^T \\mathbf{x}_n, \\beta^{-1}) \\\\\n",
    "&\\propto \\exp \\left( -\\frac{\\beta}{2} \\sum_{n=1}^N (t_n - \\mathbf{w}^T \\mathbf{x}_n)^2 \\right)\n",
    "\\\\\n",
    "&\\propto \\exp \\left( -\\frac{\\beta}{2} \\|\\mathbf{t} - \\mathbf{X} \\mathbf{w}\\|^2 \\right)\n",
    "\\end{aligned}  \\tag{5}\n",
    "$$\n",
    "\n",
    "\n",
    "Note: since the data matrix $\\mathbf{X} $ always appears in the set of conditioning variables, from this point onwards, we will drop the explicit \"$\\mathbf{X}$\" from expressions such as $ p(\\mathbf{t} | \\mathbf{X}, \\mathbf{w}, \\beta) $ to keep the notation uncluttered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log-likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To better analyze the likelihood function, we often work with its *logarithm*. Taking the logarithm simplifies the product of probabilities into a sum, thus simplifying calculations and avoiding numerical underflow.\n",
    "\n",
    "The **log-likelihood** of our model is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ln p(\\mathbf{t} | \\mathbf{w}, \\beta) \n",
    "&= \\sum_{n=1}^N \\ln \\mathcal{N}(t_n | \\mathbf{w}^\\top \\mathbf{x}_n, \\beta^{-1}) \\\\\n",
    "&= \\frac{N}{2} \\ln \\beta - \\frac{N}{2} \\ln(2\\pi) - \\beta E_D(\\mathbf{w}) \n",
    "\\end{aligned}\\tag{6}\n",
    "$$\n",
    "\n",
    "The last component of the equation $E_D(\\mathbf{w})$ is an important one - it is the **sum of squares error function** and is defined as:\n",
    "\n",
    "$$\n",
    "E_D(\\mathbf{w}) = \\frac{1}{2} \\sum_{n=1}^N \\big(t_n - \\mathbf{w}^\\top \\mathbf{x}_n\\big)^2. \\tag{7}\n",
    "$$\n",
    "\n",
    "The log-likelihood above measures how well the model parameters $\\mathbf{w}$ explain the observed data. \n",
    "\n",
    "To optimize a model, we should aim to **maximize the log-likelihood**. From **(6)**, we observe that the only component of the log-likelihood that depends on the parameters $\\mathbf{w}$ is the sum of squared errors $E_D(\\mathbf{w})$. Consequently, this **likelihood maximization** is mathematically equivalent to minimizing $E_D(\\mathbf{w})$—that is, reducing the discrepancy between the observed target values $t_n$ and the predicted values $\\mathbf{w}^\\top \\mathbf{x}_n$ as much as possible.\n",
    "\n",
    "This connection explains why **Maximum Likelihood Estimation (MLE)** is also referred to as **Ordinary Least Squares (OLS)** in the context of linear regression with Gaussian noise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequentist Approach of Obtaining $w$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To maximize the log-likelihood in the **frequentist framework**, we compute its gradient with respect to $\\mathbf{w}$:\n",
    "\n",
    "$$\n",
    "\\nabla \\ln p(\\mathbf{t} | \\mathbf{w}, \\beta) = \\sum_{n=1}^N \\big(t_n - \\mathbf{w}^\\top \\mathbf{x}_n \\big) \\mathbf{x}_n^\\top. \\tag{8}\n",
    "$$\n",
    "\n",
    "Setting this gradient to zero gives the optimal weights $\\mathbf{w}$ that minimize the sum-of-squares error:\n",
    "\n",
    "$$\n",
    "0 = \\sum_{n=1}^N t_n \\mathbf{x}_n^\\top - \\mathbf{w}^\\top \\sum_{n=1}^N \\mathbf{x}_n \\mathbf{x}_n^\\top. \\tag{9}\n",
    "$$\n",
    "\n",
    "Solving for $\\mathbf{w}$, we obtain the famous **normal equation**:\n",
    "\n",
    "$$\n",
    "\\mathbf{w}_{\\text{MLE}} = (\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top \\mathbf{t}, \\tag{10}\n",
    "$$\n",
    "\n",
    "The term $(\\mathbf{X}^\\top \\mathbf{X})^{-1} \\mathbf{X}^\\top$ is the **Moore-Penrose pseudoinverse** of $\\mathbf{X}$; it serves as a generalization of the standard matrix inverse for cases where $\\mathbf{X}$ may not be square or invertible. This pseudoinverse plays a key role in solving linear systems when $\\mathbf{X}$ is over- or under-determined. It can be thought of as the matrix that provides the best possible approximation to an inverse in such scenarios, enabling us to project the observed data into the parameter space effectively.\n",
    "\n",
    "This *frequentist* approach provides a closed-form solution for the maximum likelihood estimate of the regression weights, $\\mathbf{w}_{\\text{MLE}}$. While straightforward, it has key limitations:\n",
    "- The estimate $\\mathbf{w}_{\\text{MLE}}$ is fixed and does not account for uncertainty in the parameters.\n",
    "- It cannot incorporate prior knowledge, which becomes particularly problematic when data is scarce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Synthetic Data Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HiSOh5L6AFuh"
   },
   "source": [
    "We will now demostrate the above theory by fitting a linear regeression on synthetic data. This synthetic data $\\mathbf{x}$ have just two features ($x_1$ and $x_2$) and 25 data points. The true relationship between the data $\\mathbf{x}$ and the target variable $y_{\\text{true}}$ is given by the equation:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y_{\\text{true}} = w_0^{\\text{true}} + x_1 \\cdot w_1^{\\text{true}} + x_2 \\cdot w_2^{\\text{true}} \\tag{11}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "where $w_0^{\\text{true}} = 1$, $w_1^{\\text{true}} = 2$, and $ w_2^{\\text{true}} = 3$.\n",
    "\n",
    "The observed target values ($y_{\\text{train}}$) are generated by adding Gaussian noise to the true target values:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y = y_{\\text{true}} + \\mathcal{N}(0, \\sigma_{\\text{true}}) \\tag{12}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Here, $\\sigma_{\\text{true}} = 0.5$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "xo4qpkVhisFX",
    "outputId": "44218333-4b7d-42f7-9b02-b8f9a2caf30d"
   },
   "outputs": [],
   "source": [
    "# Creating 25 random data points containing 2 features\n",
    "N = 25\n",
    "np.random.seed(42)\n",
    "x1 = np.random.uniform(0, 1, N)\n",
    "x2 = np.random.uniform(0, 1, N)\n",
    "X = pd.DataFrame({\"x1\": x1, \"x2\": x2})\n",
    "\n",
    "# Set the true relationship between the features and the target variable\n",
    "TRUE_INTERCEPT = 1\n",
    "TRUE_SLOPES = np.array([2, 3])\n",
    "TRUE_SIGMA = 0.5\n",
    "\n",
    "# Calculating the target variables:\n",
    "# y_true (deterministic) and y_train (includes Gaussian noise)\n",
    "y_true = TRUE_INTERCEPT + np.dot(X, TRUE_SLOPES)\n",
    "y_train = y_true + np.random.normal(0, TRUE_SIGMA, size=len(X))\n",
    "\n",
    "# Combine everything into a single DataFrame\n",
    "data = pd.concat(\n",
    "    [X, pd.Series(y_true, name=\"y_true\"), pd.Series(y_train, name=\"y_train\")],\n",
    "    axis=1,\n",
    ")\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# train test split and saving\n",
    "train_data, test_data = train_test_split(data, test_size=5)\n",
    "train_data.to_csv(\"train_data.csv\")\n",
    "test_data.to_csv(\"test_data.csv\")\n",
    "\n",
    "# Display the train_data DataFrame\n",
    "style_data(test_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zUygtX7Ad698"
   },
   "source": [
    "The line chart below plots the relationship between $x_1$, $x_2$ and the targets - both the theoretical $y_{\\text{true}}$, represented by the red line, and the observed $y_{\\text{train}}$ that contains Gaussian noise, represented by the blue dots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix feature1 and feature2 constants\n",
    "x1_constant = train_data[\"x1\"].mean()\n",
    "x2_constant = train_data[\"x2\"].mean()\n",
    "\n",
    "# Recalculate the true target `y_true` for a constant x1\n",
    "y_true_fixed_x1 = (\n",
    "    TRUE_INTERCEPT + TRUE_SLOPES[0] * x1_constant + TRUE_SLOPES[1] * train_data[\"x2\"]\n",
    ")\n",
    "\n",
    "# Recalculate the true target `y_true` for a constant x2\n",
    "y_true_fixed_x2 = (\n",
    "    TRUE_INTERCEPT + TRUE_SLOPES[0] * train_data[\"x1\"] + TRUE_SLOPES[1] * x2_constant\n",
    ")\n",
    "\n",
    "# Set up the plot\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot feature1 vs. y_train with x2 constant\n",
    "axes[0].scatter(\n",
    "    train_data[\"x1\"],\n",
    "    train_data[\"y_train\"],\n",
    "    label=\"Observed `y_train` (containing noise)\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "axes[0].plot(\n",
    "    train_data[\"x1\"],\n",
    "    y_true_fixed_x2,\n",
    "    color=\"red\",\n",
    "    label=\"Theoretical `y_true`\",\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[0].set_xlabel(\"x1\")\n",
    "axes[0].set_ylabel(\"target\")\n",
    "axes[0].set_title(f\"x1 vs target\\n(x2 fixed at {x2_constant:.2f})\")\n",
    "axes[0].legend()\n",
    "\n",
    "# Plot feature2 vs. y_train with x1 constant\n",
    "axes[1].scatter(\n",
    "    train_data[\"x2\"],\n",
    "    train_data[\"y_train\"],\n",
    "    label=\"Observed `y_train` (containing noise)\",\n",
    "    alpha=0.6,\n",
    ")\n",
    "axes[1].plot(\n",
    "    train_data[\"x2\"],\n",
    "    y_true_fixed_x1,\n",
    "    color=\"blue\",\n",
    "    label=\"Theoretical `y_true`\",\n",
    "    linewidth=2,\n",
    ")\n",
    "axes[1].set_xlabel(\"x2\")\n",
    "axes[1].set_title(f\"x2 vs target\\n(x1 fixed at {x1_constant:.2f})\")\n",
    "axes[1].legend()\n",
    "\n",
    "# Improve spacing and show plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uB-IoGsjd3vA"
   },
   "source": [
    "We will also create synthetic **testing** data to evaluate the models' performance. The following code generates 10 new testing data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "X0ddXs1Ii0Yb",
    "outputId": "5f04ae6d-1dd8-475d-ac91-cc77564c3a24"
   },
   "outputs": [],
   "source": [
    "X_test = test_data[[\"x1\", \"x2\"]]\n",
    "style_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coding OLS Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From Scratch (Normal Equation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having generated the synthetic data, we are now ready to solve the problem.  \n",
    "\n",
    "To begin, we will solve the linear regression problem \"from scratch\" by applying the normal equation to compute $\\mathbf{w}_{\\text{MLE}}$, the maximum likelihood estimate of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = train_data[[\"x1\", \"x2\"]]\n",
    "y_train = train_data[\"y_train\"]\n",
    "\n",
    "# For simplicity, we'll use `X` and `y` to represent the final forms X_train and y_train\n",
    "X = np.c_[np.ones(len(X_train)), X_train]  # Add a column of ones to the features\n",
    "y = y_train.values.reshape(-1, 1)  # Reshape y to a column vector\n",
    "\n",
    "# Applying the normal equation\n",
    "X_pseudo_inverse = np.linalg.inv(X.T @ X) @ X.T\n",
    "weights = X_pseudo_inverse @ y\n",
    "\n",
    "# Extracting the intercept and slopes\n",
    "intercept = weights[0, 0]\n",
    "slopes = weights[1:].flatten()\n",
    "\n",
    "# Calculating residuals and their standard deviation\n",
    "y_pred = X @ weights\n",
    "residuals = y - y_pred\n",
    "estimated_sigma = residuals.std()\n",
    "\n",
    "\n",
    "# ================== Reporting ==================\n",
    "\n",
    "true_model_latex = rf\"\"\"\n",
    "\\text{{True data generating model:}} \\\\\n",
    "y_{{\\text{{true}}}} = {TRUE_SLOPES[0]:.2f} \\cdot x_1 +\n",
    "{TRUE_SLOPES[1]:.2f} \\cdot x_2 + {TRUE_INTERCEPT:.2f} \\\\\n",
    "\\text{{True standard deviation: }} \\sigma_{{\\text{{true}}}} = {TRUE_SIGMA:.2f}\\\\\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "estimated_model_latex_normal = rf\"\"\"\n",
    "\\text{{Estimated MLE model (using Normal equation):}} \\\\\n",
    "\\hat{{y}} = {slopes[0]:.2f} \\cdot x_1 + {slopes[1]:.2f} \\cdot x_2 + {intercept:.2f} \\\\\n",
    "\\text{{Standard deviation of residuals: }} \\hat{{\\sigma}} = {estimated_sigma:.2f}\n",
    "\"\"\"\n",
    "\n",
    "display(Math(true_model_latex))\n",
    "display(Math(estimated_model_latex_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the estimated $\\mathbf{w}_{\\text{MLE}}$ is not too far of from the true data generating model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using `statsmodels`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Manually coding the Normal Equation every time we solve a linear regression problem can quickly become tedious and error-prone.  \n",
    "\n",
    "Thankfully, the **`statsmodels`** library simplifies this process, allowing us to fit an Ordinary Least Squares (OLS) model in just two lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P7Af9sHKKdx8"
   },
   "outputs": [],
   "source": [
    "X_train_with_ones = sm.add_constant(X_train)\n",
    "ols_model = sm.OLS(y_train, X_train_with_ones).fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T65gKYllh18N"
   },
   "source": [
    "The code below extracts the parameter estimates from the `ols_model`.\n",
    "\n",
    "Notice that the estimated slopes, intercept, and standard deviation closely match those derived from the Normal equation. This is expected since **`statsmodels`** leverages a variation of the Normal equation behind the scenes to compute these estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicted values for y_train\n",
    "y_train_pred = ols_model.predict(X_train_with_ones)\n",
    "residuals = y_train_pred - y_train\n",
    "\n",
    "# ================== Reporting ==================\n",
    "\n",
    "true_model_latex = rf\"\"\"\n",
    "\\text{{True data generating model:}} \\\\\n",
    "y_{{\\text{{true}}}} = {TRUE_SLOPES[0]:.2f} \\cdot x_1 +\n",
    "{TRUE_SLOPES[1]:.2f} \\cdot x_2 + {TRUE_INTERCEPT:.2f} \\\\\n",
    "\\text{{True standard deviation: }} \\sigma = {TRUE_SIGMA:.2f}\\\\\n",
    "\"\"\"\n",
    "\n",
    "estimated_model_latex = rf\"\"\"\n",
    "\\text{{Estimated MLE model:}} \\\\\n",
    "\\hat{{y}} = {ols_model.params.iloc[1]:.2f} \\cdot x_1 +\n",
    "{ols_model.params.iloc[2]:.2f} \\cdot x_2 + {ols_model.params.iloc[0]:.2f} \\\\\n",
    "\\text{{Standard deviation of residuals: }} \\hat{{\\sigma}} = {residuals.std():.2f}\\\\\n",
    "\"\"\"\n",
    "\n",
    "# Displaying the results using LaTeX\n",
    "display(Math(true_model_latex))\n",
    "display(Math(estimated_model_latex))\n",
    "display(Math(estimated_model_latex_normal))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "evk3LGh6nEkU"
   },
   "source": [
    "\n",
    "Using the trained `ols_model`,  we can also create point predictions on the unseen `X_test` along with the corresponding confidence interval.\n",
    "\n",
    "The latter provides a range within which we expect the true parameter to lie with a certain level of confidence (e.g., 95%).\n",
    "\n",
    "In this code, we fix **`feature2`** at its mean value to isolate and visualize the influence of **`feature1`** on the target variable. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix feature2 at its mean\n",
    "x2_constant = X_test[\"x2\"].mean()\n",
    "\n",
    "# Create a new test dataset with feature2 fixed at the constant value\n",
    "X_test_fixed_x2 = X_test.copy()\n",
    "X_test_fixed_x2[\"x2\"] = x2_constant\n",
    "\n",
    "# Predict y_test using the linear model after adding constant\n",
    "predictions_fixed_x2 = ols_model.get_prediction(\n",
    "    sm.add_constant(X_test_fixed_x2, has_constant=\"add\")\n",
    ")\n",
    "pred_summary_fixed_x2 = predictions_fixed_x2.summary_frame(alpha=0.05)\n",
    "\n",
    "# Extract predicted values and confidence intervals\n",
    "y_test_pred_fixed_x2 = pred_summary_fixed_x2[\"mean\"]\n",
    "conf_int_lower_fixed_x2 = pred_summary_fixed_x2[\"obs_ci_lower\"]\n",
    "conf_int_upper_fixed_x2 = pred_summary_fixed_x2[\"obs_ci_upper\"]\n",
    "\n",
    "sorted_indices = np.argsort(X_test[\"x1\"])\n",
    "X_test_sorted = X_test[\"x1\"].iloc[sorted_indices]\n",
    "y_test_pred_sorted = y_test_pred_fixed_x2.iloc[sorted_indices]\n",
    "conf_int_lower_sorted = conf_int_lower_fixed_x2.iloc[sorted_indices]\n",
    "conf_int_upper_sorted = conf_int_upper_fixed_x2.iloc[sorted_indices]\n",
    "\n",
    "# Plot the predictions with the confidence intervals\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_test_sorted, y_test_pred_sorted, color=\"blue\", label=\"Predicted values\")\n",
    "plt.fill_between(\n",
    "    X_test_sorted,\n",
    "    conf_int_lower_sorted,\n",
    "    conf_int_upper_sorted,\n",
    "    color=\"lightblue\",\n",
    "    alpha=0.4,\n",
    "    label=\"95% Confidence Interval\",\n",
    ")\n",
    "plt.xlabel(\"x1\")\n",
    "plt.ylabel(\"Predicted Target\")\n",
    "plt.title(f\"Predictions for X_test \\n(x2 fixed at {x2_constant:.2f})\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Confidence Interval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The prediction above illustrates the frequentist concept of a **confidence interval (CI)**. A confidence interval provides a range of plausible values for an unknown parameter (such the regression coefficient) based on the observed data. For example, a 95% CI means that if we were to repeat the same experiment or sampling process many times, approximately 95% of the intervals constructed from those experiments would contain the true value of the parameter.\n",
    "\n",
    "It’s important to note that a CI does not indicate the probability that the true parameter lies within the interval for a single sample. This distinction often leads to confusion, as people sometimes interpret CIs in a probabilistic way that is closer to Bayesian reasoning.\n",
    "\n",
    "We will see later the Bayesian counterpart of CI, termed the **Bayesian credible intervals**, in contrast, do provide a probabilistic statement about the parameter itself. For instance, a 95% Bayesian credible interval directly states that there is a 95% probability that the true parameter value lies within the interval, conditioned on the data and the prior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAxBG7mR2Ar1"
   },
   "source": [
    "# Bayesian Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a solid foundation in linear regression and the frequentist approach using Maximum Likelihood Estimation (MLE), let us shift our focus to Bayesian linear regression. Bayesian linear regression estimates parameters using two sources of information: prior beliefs on those parameters and observed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "We will see that compared to OLS regression, Bayesian Linear Regression offers several key advantages:\n",
    "\n",
    "1. **Incorporation of Prior Knowledge**: Bayesian regression allows you to incorporate prior beliefs about parameters, which can improve estimates, especially in cases where data is sparse.\n",
    "\n",
    "2. **Uncertainty Quantification**: Unlike the frequentist approach which provides only single point estimates for model parameters, Bayesian linear regression outputs entire probability distributions. This allows for a richer understanding of parameter uncertainty and facilitates probabilistic predictions.\n",
    "\n",
    "\n",
    "In this section, we will explore the theoretical framework used in Bayesian linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NVVm_Idgo0Ac"
   },
   "source": [
    "Bayesian linear regression directly applies Bayes' Theorem to estimate the $\\color{green}{\\textbf{posterior distributions}}$ of the model parameters. \n",
    "\n",
    "As a reminder, here is Bayes' Theorem:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\color{green}{P(\\mathbf{w} \\mid \\mathbf{t})} &= \\frac{\\color{blue}{P(\\mathbf{t} \\mid  \\mathbf{w})} \\times \\color{orange}{P(\\mathbf{w})}}{\\color{purple}{P(\\mathbf{t})}} \\\\\n",
    "\\color{green}{\\text{posterior}} &= \\frac{\\color{blue}{\\text{likelihood}} \\times \\color{orange}{\\text{prior}}}{\\color{purple}{\\text{marginal likelihood}}}\n",
    "\\end{align}\n",
    "\\tag{13}\n",
    "$$ \n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\color{orange}{P(\\mathbf{w})}$ is the $\\color{orange}{\\textbf{prior}}$ for parameters $\\mathbf{w}$, reflecting our beliefs about $\\mathbf{w}$ before observing the data $(\\mathbf{t}, \\mathbf{X})$. For instance, if we assume most predictors should have little influence, we can set $\\color{orange}{P(\\mathbf{w})}$ to be a Gaussian centered at zero with small standard deviation, which represents our high certainty that their values are close to zero. Technically, we can also have $\\color{orange}{P(\\beta)}$ - the prior for the precision parameter $\\beta$ (inverse variance of the noise). However, to simplify our discussion, we'll assume $\\beta$ is known, so this term is constant and can be ignored.  \n",
    "\n",
    "\n",
    "- $\\color{green}{P(\\mathbf{w}\\mid\\mathbf{t}})$ represents the $\\color{green}{\\textbf{posterior}}$ for $\\mathbf{w}$ given the observed data $(\\mathbf{t}, \\mathbf{X})$. It combines the prior and the likelihood and quantifies our belief about $\\mathbf{w}$ *after* observing the data. For example, if we initially believe a slope $w_1$ should be small due to prior domain knowledge but the data $\\mathbf{X}$ strongly suggests otherwise, the posterior would balance these two sources of information.\n",
    "\n",
    "\n",
    "\n",
    "- $\\color{blue}{P(\\mathbf{t} \\mid \\mathbf{w})}$ is the $\\color{blue}{\\textbf{likelihood}}$ of the target data $\\mathbf{t}$ given the input data $\\mathbf{X}$, parameters $\\mathbf{w}$, and noise precision $\\beta$. It measures how well a particular set of parameters $\\mathbf{w}$ explains the observed target values $\\mathbf{t}$. As mentioned above, we'll assume that each data point $(\\mathbf{x}_n, t_n)$ is drawn independently from a Gaussian.\n",
    "\n",
    "\n",
    "- $\\color{purple}{P(\\mathbf{t})}$ is the $\\color{purple}{\\textbf{marginal likelihood}}$, which ensures that the posterior  sums to one. It integrates out all possible parameter values $\\mathbf{w}$ from the joint probability. The marginal likelihood is particularly important for Bayesian model comparison because it quantifies how well a model as a whole explains the data independent of any specific parameter settings.\n",
    "\n",
    "A notational reminder: As the data matrix $\\mathbf{X}$ and the known precision parameter $\\beta$ always appear in the set of conditioning variables, we have dropped the explicit $\\mathbf{X}$ and $\\beta$ from expressions such as the likelihood $\\color{blue}{P(\\mathbf{t} \\mid \\mathbf{X}, \\mathbf{w}, \\beta)}$ to keep the notation uncluttered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Challenge of Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As demonstrated above, Bayesian inference aims to determine the $\\color{green}{\\textbf{posterior}}$ $\\color{green}{P(\\mathbf{w} \\mid \\mathbf{t})}$ by applying Bayes' theorem. This posterior encapsulates our updated belief about the parameters $\\mathbf{w}$ after observing the data $\\mathbf{t}$ and $\\mathbf{X}$.\n",
    "\n",
    "However, in many real-world applications, computing the posterior directly is computationally challenging. The difficulty arises from the denominator in Bayes' formula—the $\\color{purple}{\\textbf{marginal likelihood}}$ $\\color{purple}{P(\\mathbf{t})}$—whose calculation requires integrating over the parameter space to marginalize $\\mathbf{w}$ from the $\\color{blue}{\\textbf{likelihood}}$ $\\color{blue}{P(\\mathbf{t} \\mid \\mathbf{w})}$:\n",
    "\n",
    "$$\n",
    "\\color{purple}{P(\\mathbf{t})} = \\int \\color{blue}{P(\\mathbf{t} \\mid \\mathbf{w})} \\cdot \\color{orange}{P(\\mathbf{w})} \\, d\\mathbf{w}.\n",
    "\\tag{14}\n",
    "$$\n",
    "\n",
    "For high-dimensional parameter spaces, this integral is often analytically intractable due to its complexity. In such cases, we must rely on numerical strategies to approximate the posterior. \n",
    "\n",
    "\n",
    "Below, we'll introduce some strategies for obtaining/approximating the posterior. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Strategies for Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To obtain or approximate the posterior distribution, we can employ several strategies. The most common approaches include **conjugate priors**, **Markov Chain Monte Carlo (MCMC)**, and **Variational Inference (VI)**, which are briefly introduced below.\n",
    "\n",
    "\n",
    "\n",
    "### Conjugate Priors\n",
    "\n",
    "A conjugate prior is a type of prior distribution that makes Bayesian inference much easier. When paired with a specific likelihood, this prior ensures the posterior distribution stays in the same family as the prior  (hence the name \"conjugate\"). This property of 'staying in the same family' means we can compute the posterior analytically without needing any numerical methods!\n",
    "\n",
    "For example, in Bayesian linear regression with a Gaussian likelihood, if we were to use a Gaussian prior, the resulting posterior is also Gaussian. This property of \"staying in the same family\" greatly simplifies Bayesian computations and allows us to obtain an analytical form of the posterior. \n",
    "\n",
    "The second notebook in this series delves into conjugacy in more details.\n",
    "\n",
    "### Markov Chain Monte Carlo (MCMC)\n",
    "\n",
    "Markov Chain Monte Carlo (MCMC) is a group of algorithms designed to sample from the posterior. It works by building a Markov chain where the posterior serves as the equilibrium distribution. Once enough samples are generated, they can be used to numerically approximate the posterior and derive insights about the parameters.\n",
    "\n",
    "We'll dive deeper into how MCMC works and its practical applications in the third notebook of this series.\n",
    "\n",
    "###  Variational Inference (VI)\n",
    "\n",
    "\n",
    "Variational Inference (VI) offers a clever way to approximate the $\\color{green}{\\textbf{posterior }P(\\mathbf{w} \\mid \\mathbf{t})}$. Instead of using sampling, VI seeks $\\color{salmon}\\textbf{a simpler distribution  }{q(\\mathbf{w})}$ from a predefined family that closely approximates posterior. This approximation is done by minimizing the **Kullback-Leibler (KL) divergence** (i.e. the difference) between $\\color{salmon}{q(\\mathbf{w})}$ and the posterior:\n",
    "\n",
    "$$\n",
    "\\color{salmon}{q^*(\\mathbf{w})} \\color{black}{= \\arg \\min_{\\color{black}{q}} \\text{KL}(\\color{salmon}{q(\\mathbf{w})} \\, \\color{black}{\\parallel} \\, \\color{green}{P(\\mathbf{w} \\mid \\mathbf{t})}}\\color{black}{)}.\n",
    "$$\n",
    "\n",
    "In short, VI turns Bayesian inference into a neat optimization problem, making it faster and more scalable.\n",
    "\n",
    "We'll explore this approach in more depth in the fourth notebook of this series.\n",
    "\n",
    "### Comparison of Strategies\n",
    "\n",
    "| **Method**               | **Advantages**                                      | **Disadvantages**                                       |\n",
    "|---------------------------|----------------------------------------------------|--------------------------------------------------------|\n",
    "| **Conjugate Priors**      | Analytically tractable; computationally efficient  | Limited flexibility in prior/likelihood combinations   |\n",
    "| **MCMC**                 | Flexible and can theoretically approximate any posterior  | Can be computationally expensive             |\n",
    "| **Variational Inference** | Computationally efficient as it uses deterministic optimization | May underestimate uncertainty; <br> Result depends on the choice of approximant $q(\\mathbf{w})$ |\n",
    "\n",
    "Each method has trade-offs, and the choice of approach depends on the complexity of the model, the size of the data, and the computational resources available.\n",
    "A more detailed discussion on these methods will be provided in subsequent notebooks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cNKM7EPXJXij"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KzuX_hc8vsD7"
   },
   "source": [
    "- [Bishop - Pattern Recognition and Machine Learning (2006)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) - A comprehensive reference on machine learning theory and Bayesian methods by Christopher M. Bishop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "# Credits\n",
    "\n",
    "Notebook creation: `meraldoantonio`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
