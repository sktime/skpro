{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "In this second notebook, we'll dive deeper into the concept of conjugate priors in Bayesian linear regression (BLR).\n",
    "\n",
    "We'll see how by using conjugate distributions, we can derive posterior distributions analytically and thus simplifying Bayesian inference. \n",
    "\n",
    "After providing the theory, we will see how this inference can be done using `skpro`'s `BayesianConjugateLinearRegressor` class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "# Imports and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from IPython.display import Math, display\n",
    "from utils import style_data\n",
    "\n",
    "from skpro.regression.bayesian import BayesianConjugateLinearRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "# Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Bayesian linear regression: recap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "As mentioned in the first notebook, **Bayesian linear regression (BLR)** is a probabilistic approach to linear regression where prior beliefs about the model parameters are combined with observed data to compute posterior distributions. Unlike traditional linear regression, which only provides point estimates for parameters and predictions, BLR offers a complete distribution for them, providing an intuitive way to reason about uncertainty in both parameters and predictions.\n",
    "\n",
    "As a reminder, BLR builds on Bayes' theorem:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\color{green}{P(\\mathbf{w} \\mid \\mathbf{t})} &= \\frac{\\color{blue}{P(\\mathbf{t} \\mid \\mathbf{w})} \\times \\color{orange}{P(\\mathbf{w})}}{\\color{purple}{P(\\mathbf{t})}} \\\\\n",
    "\\color{green}{\\text{posterior}} &= \\frac{\\color{blue}{\\text{likelihood}} \\times \\color{orange}{\\text{prior}}}{\\color{purple}{\\text{marginal likelihood}}}\n",
    "\\end{align*}\n",
    "\\tag{1}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $\\color{orange}{P(\\mathbf{w})}$ is the $\\color{orange}{\\textbf{prior}}$ for parameters $\\mathbf{w}$, reflecting our beliefs about $\\mathbf{w}$ before observing the data. \n",
    "\n",
    "\n",
    "- $\\color{green}{P(\\mathbf{w} \\mid \\mathbf{t})}$ represents the $\\color{green}{\\textbf{posterior}}$ for $\\mathbf{w}$ given the observed data $(\\mathbf{t}, \\mathbf{X})$. It combines the prior and the likelihood and quantifies our belief about $\\mathbf{w}$ *after* observing the data. \n",
    "\n",
    "\n",
    "- $\\color{purple}{P(\\mathbf{t})}$ is the $\\color{purple}{\\textbf{marginal likelihood}}$, which ensures that the posterior  sums to one. \n",
    "\n",
    "\n",
    "- $\\color{blue}{P(\\mathbf{t} \\mid \\mathbf{w})}$  is the $\\color{blue}{\\textbf{likelihood}}$ of the target data $\\mathbf{t}$ given the input data $\\mathbf{X}$, parameters $\\mathbf{w}$, and noise precision $\\beta$. It measures how well a particular set of parameters $\\mathbf{w}$ explains the observed target values $\\mathbf{t}$. Assuming each data point $(\\mathbf{x}_n, t_n)$ is drawn independently, the likelihood is:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\color{blue}{P(\\mathbf{t} \\mid \\mathbf{w})} &= \\prod_{n=1}^N \\mathcal{N}(t_n \\mid \\mathbf{w}^T \\mathbf{x}_n, \\beta^{-1}) \\\\\n",
    "&\\propto \\exp \\left( -\\frac{\\beta}{2} \\sum_{n=1}^N (t_n - \\mathbf{w}^T \\mathbf{x}_n)^2 \\right) \\\\\n",
    "&\\propto \\exp \\left( -\\frac{\\beta}{2} \\|\\mathbf{t} - \\mathbf{X} \\mathbf{w}\\|^2 \\right) \\tag{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "A notational reminder: As the data matrix $\\mathbf{X}$ and the known precision parameter $\\beta$ always appear in the set of conditioning variables, we have dropped the explicit $\\mathbf{X}$ and $\\beta$ from expressions such as the likelihood $\\color{blue}{P(\\mathbf{t} \\mid \\mathbf{X}, \\mathbf{w}, \\beta)}$ to keep the notation uncluttered.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Conjugacy and Conjugate Prior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "A **conjugate prior** is a prior distribution that, when combined with a specific likelihood, ensures that the posterior distribution belongs to the same family as the prior. We'll soon see that this property of \"staying in the same family\" greatly simplifies Bayesian computations!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "*So, what would be the conjugate prior in our case?*\n",
    "\n",
    "To answer this, let’s revisit the likelihood. As mentioned earlier, the likelihood $ \\color{blue}{P(\\mathbf{t} \\mid \\mathbf{w})} $ follows a Gaussian distribution.\n",
    "\n",
    "To ensure conjugacy and simplify computation, we choose a multivariate Gaussian distribution as the prior for $ \\mathbf{w} $. This choice leverages a key property of Gaussian distributions: *the product of two Gaussian distributions is also a Gaussian*.\n",
    "\n",
    "Let’s now define the prior to set the stage for the analytical derivation that follows. \n",
    "\n",
    "The multivariate Gaussian prior is given by:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\color{orange}{P(\\mathbf{w})} &= \\mathcal{N}(\\mathbf{w} \\mid \\mathbf{m}_0, \\mathbf{S}_0) \\\\\n",
    "&\\propto \\exp \\left( -\\frac{1}{2} (\\mathbf{w} - \\mathbf{m}_0)^\\top \\mathbf{S}_0^{-1} (\\mathbf{w} - \\mathbf{m}_0) \\right) \\tag{3}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Here:\n",
    "- $ \\mathbf{m}_0 $ is the **mean vector** of the multivariate Gaussian prior of the the regression coefficients $\\mathbf{w}$. <br>\n",
    "$ \\mathbf{m}_0 $ has a shape of $(D, 1)$, where $D$ is the number of features.\n",
    "- $ \\mathbf{S}_0 $ is the **covariance matrix** of the prior, with shape $ (D, D) $.\n",
    "\n",
    "Both $ \\mathbf{m}_0 $ and $ \\mathbf{S}_0 $ should be selected based on prior knowledge or assumptions about the regression coefficients.\n",
    "\n",
    "> **Note:** Do not confuse $\\mathbf{S}_0$, the prior covariance matrix of the parameter $\\mathbf{w}$, with $\\beta$, which represents the precision (inverse variance) of the noise term in regression. In this framework, for simplicity, we assume $\\beta$ is known. If it is unknown, an alternative conjugate framework, the Multivariate Normal-Wishart distribution, must be used.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Conjugate Posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "According to the Bayes formula, the $\\color{green}{\\textbf{posterior}}$ is proportional to the product of the $\\color{blue}{\\textbf{likelihood}}$ and $\\color{orange}{\\textbf{prior}}$:\n",
    "<br>\n",
    "\n",
    "$$\n",
    "\\color{green}{P(\\mathbf{w} | \\mathbf{t})} \\propto \\color{blue}{\\exp \\left( -\\frac{\\beta}{2} \\|\\mathbf{t} - \\mathbf{X} \\mathbf{w}\\|^2 \\right)} \\color{orange}{\\exp \\left( -\\frac{1}{2} (\\mathbf{w} - \\mathbf{m}_0)^T \\mathbf{S}_0^{-1} (\\mathbf{w} - \\mathbf{m}_0) \\right)} \\tag{4}\n",
    "$$\n",
    "\n",
    "After expanding the terms in the exponents and completing the square, we obtain a posterior that's also a multivariate Gaussian:\n",
    "\n",
    "$$\n",
    "\\color{green}{P(\\mathbf{w} | \\mathbf{t})} = \\mathcal{N}(\\mathbf{w} | \\mathbf{m}_N, \\mathbf{S}_N) \\tag{5}\n",
    "$$\n",
    "\n",
    "In this formula,\n",
    "$\\mathbf{S}_N$ is the *covariance* of our posterior Gaussian distribution. <br> Its inverse (i.e. the  *precision* of the posterior) can be conveniently calculated from the prior and the data by simply using the formula below:\n",
    "\n",
    "  $$\n",
    "  \\mathbf{S}_N^{-1} = \\mathbf{S}_0^{-1} + \\beta \\mathbf{X}^T \\mathbf{X} \\tag{6}\n",
    "  $$\n",
    "  \n",
    "We see that the formula lends itself to the following intution:\n",
    "\n",
    "- The posterior precision $\\mathbf{S}_N^{-1}$ is the sum of the prior-derived precision and data-derived precision.\n",
    "- The prior precision $\\mathbf{S}_0^{-1}$ reflects initial uncertainty in the weights $\\mathbf{w}$.\n",
    "- On the other hand, the data-derived precision $\\beta \\mathbf{X}^T \\mathbf{X}$ reflects the improvement in precision coming from observing data $\\mathbf{X}$, adjusted by the noise precision $\\beta$.\n",
    "\n",
    "\n",
    "Meanwhile, $\\mathbf{m}_N$ is the mean of our posterior. It is calculated using the following formula:\n",
    "\n",
    "  $$\n",
    "  \\mathbf{m}_N = \\mathbf{S}_N \\left( \\mathbf{S}_0^{-1} \\mathbf{m}_0 + \\beta \\mathbf{X}^T \\mathbf{t} \\right) \\tag{7}\n",
    "  $$\n",
    "\n",
    " We note that $\\mathbf{m}_N$ is essentially a **weighted average** of the prior mean vector, $\\mathbf{m}_0$, and the observed data (represented by $\\mathbf{X}^T \\mathbf{t}$). \n",
    "\n",
    "Another observation: if we set an infinitively broad (i.e. completely uninformative) prior with a zero lprecision $\\mathbf{S}_0$, we see that the Bayesian posterior estimate reduces to the frequentist $\\mathbf{w}_{\\text{MLE}}$ obtained through the normal equation:\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathbf{S}_N^{-1} = \\mathbf{0} + \\beta \\mathbf{X}^T \\mathbf{X}  \\tag{Assuming $\\mathbf{S}_0^{-1} = \\mathbf{0}$}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{S}_N = \\left( \\beta \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\tag{Inversing $\\mathbf{S}_N^{-1}$}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbf{m}_N &= \\left( \\beta \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\beta \\mathbf{X}^T \\mathbf{t} \\tag{Substituting $\\mathbf{S}_N^{-1}$ into $\\mathbf{m}_N$} \\\\\n",
    "&= \\left( \\mathbf{X}^T \\mathbf{X} \\right)^{-1} \\mathbf{X}^T \\mathbf{t} \\tag{Normal Equation recovered!} \\\\\n",
    "&= \\mathbf{w}_{\\text{MLE}} \\notag\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## Posterior Predictive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "Our ultimate goal is to get the posterior predictive distribution of a new target $t$ given a new input $\\mathbf{x}$:\n",
    "\n",
    "$$\n",
    "p(t | \\mathbf{x}, \\mathbf{X}, \\mathbf{t}) = \\mathcal{N}(t | m(\\mathbf{x}), s^2(\\mathbf{x}))\n",
    "$$\n",
    "\n",
    "As the notation suggests, this distribution depends on the training data ($\\mathbf{X}$ and $\\mathbf{t}$) used to fit the model.\n",
    "\n",
    "This posterior predictive distribution is a univariate Gaussian with mean $m(\\mathbf{x})$ and variance $s^2(\\mathbf{x})$, both of which depend on the given input $\\mathbf{x}$.\n",
    "\n",
    "\n",
    "### Predictive Mean\n",
    "\n",
    "The predictive mean $m(\\mathbf{x})$ is given by:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "m(\\mathbf{x}) &= \\mathbf{x}^T \\beta S_N \\mathbf{X}^T \\mathbf{t} \\\\\n",
    "&= \\mathbf{x}^T \\mathbf{m}_N \\tag{8}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We note that this predictive mean is very simple: it is simply a projection of incoming data point $\\mathbf{x}$ onto the posterior mean $\\mathbf{m}_N$.\n",
    "\n",
    "\n",
    "### Predictive Variance\n",
    "\n",
    "The predictive variance $s^2(\\mathbf{x})$ is given by:\n",
    "$$\n",
    "s^2(\\mathbf{x}) = \\beta^{-1} + \\mathbf{x}^T S_N \\mathbf{x} \\tag{9}\n",
    "$$\n",
    "\n",
    "\n",
    "Predictive variance quantifies model confidence, increasing in regions far from the training data or in uncertain directions. From the formula, we note that this uncertainty depends on the **position** and **direction** of the incoming data point $\\mathbf{x}$:\n",
    "\n",
    "- If it's close to fitted training data: $\\mathbf{x}^T S_N \\mathbf{x}$ is small near training data, where the model is confident.\n",
    "- On the other hand, if it is far from fitted training data: $\\mathbf{x}^T S_N \\mathbf{x}$ grows as $\\mathbf{x}$ moves away, reflecting increased uncertainty.\n",
    "- Lastly, larger $\\|\\mathbf{x}\\|$ increases $\\mathbf{x}^T S_N \\mathbf{x}$, leading to higher variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "# Application"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "The above framework is implemented by the `BayesianConjugateLinearRegressor` class from `skpro`. In this section, we'll take a look at its usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "We will use the same synthetic dataset we generated in the first notebook.\n",
    "\n",
    "As a reminder, the true parameter values are:\n",
    "- Intercept $w_0$ = 1\n",
    "- First regression coefficient $w_1$ = 2\n",
    "- Second regression coefficient $w_2$ = 3\n",
    "- Noise variance $\\sigma$ = 0.5; in other words, noise *precision* $\\beta$ is 4. These values are assumed to be known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(\"train_data.csv\", index_col=0)\n",
    "X_train = train_data[[\"x1\", \"x2\"]]\n",
    "y_train = train_data[\"y_train\"]\n",
    "\n",
    "test_data = pd.read_csv(\"test_data.csv\", index_col=0)\n",
    "X_test = test_data[[\"x1\", \"x2\"]]\n",
    "\n",
    "style_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "## Instantiation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "To instantiate a `BayesianConjugateLinearRegressor` model, we need to define a multivariate Gaussian prior for the regression coefficients using the following parameters:\n",
    "\n",
    "1. **`coefs_prior_mu`**: The prior mean vector ($\\mathbf{m}_0$). <br>\n",
    "Suppose that based on prior knowledge, we estimate that the regression coefficients are likely around 4 and 5. (Note: as mentioned earlier, the true values of $w_1$ and $w_2$ are actually $2$ and $3$, respectively, so this assumption is slightly off). For the intercept, we'll (correctly) assume it to be close to 1. Hence, we select the prior mean vector as follows; note that the first element of the vector is the intercept:\n",
    "   $$\n",
    "   \\mathbf{m}_0 = \n",
    "   \\begin{bmatrix}\n",
    "   1 \\\\\\\\ \n",
    "   4 \\\\\\\\ \n",
    "   5\n",
    "   \\end{bmatrix}\n",
    "   $$\n",
    "\n",
    "2. **`coefs_prior_cov`** <br>\n",
    " The prior covariance matrix ($\\mathbf{S}_0$). To keep the model simple, we'll assume that the intercept and coefficients are independent and have equal variance. This leads us to select an identity matrix for **`coefs_prior_cov`**, expressed as:\n",
    "\n",
    "$$\n",
    "\\mathbf{S}_0 = \n",
    "\\begin{bmatrix}\n",
    "1 & 0 & 0 \\\\\\\\ \n",
    "0 & 1 & 0 \\\\\\\\\n",
    "0 & 0 & 1\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Additionally, we need to specify **`noise_precision`**, the known precision ($\\beta$) of the Gaussian noise in the data. For this example, we'll assume we know the true noise precision value which is $4$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "COEFS_PRIOR_MU = np.array(\n",
    "    [\n",
    "        [1.0],  # Prior for intercept and coefficients;\n",
    "        [4.0],  # the 1st value is the intercept\n",
    "        [5.0],\n",
    "    ]\n",
    ")  # the 2nd and 3rd are the mean priors for w1 and w2\n",
    "COEFS_PRIOR_COV = np.eye(3)  # Covariance matrix which is Identity\n",
    "NOISE_PRECISION = 4\n",
    "\n",
    "model = BayesianConjugateLinearRegressor(\n",
    "    coefs_prior_mu=COEFS_PRIOR_MU,\n",
    "    coefs_prior_cov=COEFS_PRIOR_COV,\n",
    "    noise_precision=NOISE_PRECISION,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "Before fitting the model to our data, let's visualize the shape of our multivariate Gaussian prior for the coefficients $w_1$ and $w_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "COL_NAMES = [\"Intercept\", \"w1\", \"w2\"]\n",
    "display(Math(r\"\\text{Mean of the prior for regression coefficents } (\\mathbf{m}_0):\"))\n",
    "display(style_data(pd.DataFrame(COEFS_PRIOR_MU, columns=[\"Value\"], index=COL_NAMES)))\n",
    "\n",
    "display(\n",
    "    Math(r\"\\text{Covariance of the prior for regression coefficents } (\\mathbf{S}_0):\")\n",
    ")\n",
    "display(style_data(pd.DataFrame(COEFS_PRIOR_COV, columns=COL_NAMES, index=COL_NAMES)))\n",
    "\n",
    "TRUE_W1 = 2\n",
    "TRUE_W2 = 3\n",
    "\n",
    "# Plot the Gaussian distribution: Contour and 3D\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Generate a grid of points\n",
    "x1, x2 = np.linspace(0, 8, 100), np.linspace(0, 8, 100)\n",
    "X1, X2 = np.meshgrid(x1, x2)\n",
    "pos = np.dstack((X1, X2))\n",
    "\n",
    "\n",
    "def multivariate_gaussian(pos, mu, cov):\n",
    "    # Helper function to compute the multivariate Gaussian PDF\n",
    "    n = mu.shape[0]\n",
    "    diff = pos - mu.flatten()\n",
    "    inv_cov = np.linalg.inv(cov)\n",
    "    exponent = -0.5 * np.einsum(\"...i,ij,...j->...\", diff, inv_cov, diff)\n",
    "    return (1.0 / np.sqrt((2 * np.pi) ** n * np.linalg.det(cov))) * np.exp(exponent)\n",
    "\n",
    "\n",
    "# Extract the w1 and w2\n",
    "Z = multivariate_gaussian(pos, COEFS_PRIOR_MU[1:], COEFS_PRIOR_COV[1:, 1:])\n",
    "\n",
    "# Contour plot\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.contour(X1, X2, Z, levels=10, cmap=\"viridis\")\n",
    "ax1.set_title(\"Contour Plot of the Bivariate Gaussian Prior\")\n",
    "ax1.set_xlabel(\"$w_1$\")\n",
    "ax1.set_ylabel(\"$w_2$\")\n",
    "ax1.plot(TRUE_W1, TRUE_W2, marker=\"*\", color=\"red\", markersize=10, label=\"True Value\")\n",
    "ax1.annotate(\n",
    "    \"True Value (2,3)\",\n",
    "    (TRUE_W1, TRUE_W2),\n",
    "    textcoords=\"offset points\",\n",
    "    xytext=(5, -18),\n",
    "    ha=\"center\",\n",
    "    color=\"red\",\n",
    ")\n",
    "ax1.grid(True)\n",
    "\n",
    "# 3D plot\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\", box_aspect=(1, 1, 0.5))\n",
    "ax2.plot_surface(X1, X2, Z, cmap=\"viridis\", edgecolor=\"none\", alpha=0.7)\n",
    "ax2.scatter(TRUE_W1, TRUE_W2, 0, marker=\"*\", color=\"red\", s=100, label=\"True Value\")\n",
    "ax2.set_title(\"3D Plot of the Bivariate Gaussian Prior\")\n",
    "ax2.set_xlabel(\"$w_1$\")\n",
    "ax2.set_ylabel(\"$w_2$\")\n",
    "ax2.set_zticklabels([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Like other estimators in the `skpro` family, the model is fitted easily with a single `.fit` call, which requires `X_train` and `y_train` as inputs.\n",
    "\n",
    "If an intercept is required, a column of ones should be added to the feature matrix. This can be easily achieved using the `add_constant` function from the `statsmodels` library.\n",
    "\n",
    "The `fit` method calculates the posterior using the conjugate formula elaborated above.\n",
    "\n",
    "After fitting, the posterior becomes available through its parameters: `_coefs_posterior_mu` (mean) and `_coefs_posterior_cov` (covariance)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_with_ones = sm.add_constant(\n",
    "    X_train, prepend=True\n",
    ")  # prepending a constant of ones\n",
    "style_data(X_train_with_ones.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train_with_ones, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "As shown below, the posterior retains the same shape as the prior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "model._coefs_posterior_mu.shape == model._coefs_prior_mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access posterior mean and covariance\n",
    "posterior_mu = model._coefs_posterior_mu.ravel()  # Flatten to match dimensions\n",
    "posterior_cov = model._coefs_posterior_cov\n",
    "\n",
    "display(Math(r\"\\text{Mean of the regression coefficients posterior } (\\mathbf{m}_N):\"))\n",
    "display(\n",
    "    style_data(\n",
    "        pd.DataFrame(posterior_mu, columns=[\"Value\"], index=[\"Intercept\", \"w1\", \"w2\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    Math(r\"\\text{Covariance of the regression coefficents posterior } (\\mathbf{S}_N):\")\n",
    ")\n",
    "display(\n",
    "    style_data(\n",
    "        pd.DataFrame(\n",
    "            posterior_cov,\n",
    "            columns=[\"Intercept\", \"w1\", \"w2\"],\n",
    "            index=[\"Intercept\", \"w1\", \"w2\"],\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "The visualization reveals that the posterior is narrower than the prior (indicating reduced uncertainty) and it is centered closer to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Compute the posterior density of w1 and w2\n",
    "Z = multivariate_gaussian(pos, posterior_mu[1:], posterior_cov[1:, 1:])\n",
    "\n",
    "# Contour plot\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.contour(X1, X2, Z, levels=10, cmap=\"viridis\")\n",
    "ax1.set_title(\"Contour Plot of the Bivariate Gaussian Posterior\")\n",
    "ax1.set_xlabel(\"$w_1$\")\n",
    "ax1.set_ylabel(\"$w_2$\")\n",
    "ax1.plot(TRUE_W1, TRUE_W2, marker=\"*\", color=\"red\", markersize=10, label=\"True Value\")\n",
    "ax1.annotate(\n",
    "    \"True Value (2, 3)\",\n",
    "    xy=(TRUE_W1, TRUE_W2),\n",
    "    xytext=(50, 50),\n",
    "    textcoords=\"offset points\",\n",
    "    ha=\"center\",\n",
    "    color=\"red\",\n",
    "    arrowprops=dict(arrowstyle=\"->\", color=\"red\", lw=1.5, alpha=0.5),\n",
    ")\n",
    "ax1.grid(True)\n",
    "\n",
    "# 3D plot\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\", box_aspect=(1, 1, 0.5))\n",
    "ax2.plot_surface(X1, X2, Z, cmap=\"viridis\", edgecolor=\"none\", alpha=0.7)\n",
    "ax2.scatter(TRUE_W1, TRUE_W2, 0, marker=\"*\", color=\"red\", s=100, label=\"True Value\")\n",
    "ax2.set_title(\"3D Plot of the Bivariate Gaussian Posterior\")\n",
    "ax2.set_xlabel(\"$w_1$\")\n",
    "ax2.set_ylabel(\"$w_2$\")\n",
    "ax2.set_zticklabels([])\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "## Update"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35",
   "metadata": {},
   "source": [
    "One significant advantage of the Bayesian approach is its simplicity in handling updates, i.e., retraining the model when new data becomes available.\n",
    "\n",
    "First, we'll generate additional synthetic training data.\n",
    "\n",
    "As before, a column of ones must be prepended to the feature matrix to include the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a new random data point for X_train_update\n",
    "np.random.seed(43)\n",
    "N = 40\n",
    "x1_new = np.random.uniform(0, 1, 40)\n",
    "x2_new = np.random.uniform(0, 1, 40)\n",
    "\n",
    "X_train_update = pd.DataFrame({\"x1\": x1_new, \"x2\": x2_new})\n",
    "X_train_update_with_ones = sm.add_constant(\n",
    "    X_train_update, prepend=True\n",
    ")  # prepending a constant of ones\n",
    "\n",
    "# Set the true relationship between the features and the target variable\n",
    "TRUE_INTERCEPT = 1\n",
    "TRUE_SLOPES = np.array([2, 3])\n",
    "TRUE_SIGMA = 0.5\n",
    "\n",
    "# Calculate y_true and y_train for the new data point\n",
    "y_true_new = TRUE_INTERCEPT + np.dot(X_train_update, TRUE_SLOPES)\n",
    "X_train_update[\"x0\"] = 1\n",
    "y_train_update = pd.Series(y_true_new + np.random.normal(0, TRUE_SIGMA, size=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "We'll then perform the update with a single call to the `update` method.\n",
    "\n",
    "The `update`method applies the same conjugacy framework, treating the posterior from the previous training as the new prior and updating it with the additional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.update(X_train_update_with_ones, y_train_update)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "The visualization below shows that the posterior after the update becomes even narrower and moves even closer to the true value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access posterior mean and covariance\n",
    "posterior_mu = model._coefs_posterior_mu.ravel()  # Flatten to match dimensions\n",
    "posterior_cov = model._coefs_posterior_cov\n",
    "\n",
    "display(Math(r\"\\text{Mean of the regression coefficents posterior } (\\mathbf{m}_N):\"))\n",
    "display(\n",
    "    style_data(\n",
    "        pd.DataFrame(posterior_mu, columns=[\"Value\"], index=[\"Intercept\", \"w1\", \"w2\"])\n",
    "    )\n",
    ")\n",
    "\n",
    "display(\n",
    "    Math(r\"\\text{Covariance of the regression coefficents posterior } (\\mathbf{S}_N):\")\n",
    ")\n",
    "display(\n",
    "    style_data(\n",
    "        pd.DataFrame(\n",
    "            posterior_cov,\n",
    "            columns=[\"Intercept\", \"w1\", \"w2\"],\n",
    "            index=[\"Intercept\", \"w1\", \"w2\"],\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "# Plot the posterior Gaussian distribution: Contour and 3D\n",
    "fig = plt.figure(figsize=(14, 6))\n",
    "\n",
    "# Compute the posterior density of w1 and w2\n",
    "Z = multivariate_gaussian(pos, posterior_mu[1:], posterior_cov[1:, 1:])\n",
    "\n",
    "# Contour plot\n",
    "ax1 = fig.add_subplot(121)\n",
    "ax1.contour(X1, X2, Z, levels=10, cmap=\"viridis\")\n",
    "ax1.set_title(\"Contour Plot of the Bivariate Gaussian Posterior after Update\")\n",
    "ax1.set_xlabel(\"$w_1$\")\n",
    "ax1.set_ylabel(\"$w_2$\")\n",
    "ax1.plot(TRUE_W1, TRUE_W2, marker=\"*\", color=\"red\", markersize=10, label=\"True Value\")\n",
    "ax1.annotate(\n",
    "    \"True Value (2,3)\",\n",
    "    (TRUE_W1, TRUE_W2),\n",
    "    textcoords=\"offset points\",\n",
    "    xytext=(3, -28),\n",
    "    ha=\"center\",\n",
    "    color=\"red\",\n",
    ")\n",
    "ax1.grid(True)\n",
    "\n",
    "# 3D plot\n",
    "ax2 = fig.add_subplot(122, projection=\"3d\", box_aspect=(1, 1, 0.5))\n",
    "ax2.plot_surface(X1, X2, Z, cmap=\"viridis\", edgecolor=\"none\", alpha=0.7)\n",
    "ax2.scatter(TRUE_W1, TRUE_W2, 0, marker=\"*\", color=\"red\", s=100, label=\"True Value\")\n",
    "ax2.set_title(\"3D Plot of the Bivariate Gaussian Posterior after Update\")\n",
    "ax2.set_xlabel(\"$w_1$\")\n",
    "ax2.set_ylabel(\"$w_2$\")\n",
    "ax2.set_zticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "## (Probabilistic) Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42",
   "metadata": {},
   "source": [
    "With the fitted model, we can now make predictions. Below is the test dataset, `X_test`, which will be used for generating predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "style_data(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "Probabilistic predictions are made using the `predict_proba` method, which requires `X_test` as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_with_ones = sm.add_constant(\n",
    "    X_test, prepend=True\n",
    ")  # prepending a constant of ones\n",
    "y_test_pred_proba = model.predict_proba(X_test_with_ones)\n",
    "y_test_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "### Plotting posterior predictive PDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The prediction output, `y_test_pred_proba`, is an instance of `skpro`'s `Normal` distribution, containing the same number of data points as `X_test`. \n",
    "\n",
    "A key advantage of this Bayesian estimator is that each prediction is a Normal distribution, making it straightforward to work with. \n",
    "\n",
    "For example, we can easily plot the probability density function of the predictions using `plot` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = y_test_pred_proba.plot(\"pdf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": [
    "### Predictive Credible Intervals"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "One great thing about using probabilities in predictions is how easily we can calculate **predictive credible intervals**. These are the Bayesian version of confidence intervals in frequentist statistics.\n",
    "\n",
    "A credible interval gives you a range where the true prediction is likely to land, based on a specific level of certainty. For instance, a 95% predictive credible interval means there’s a 95% chance that the prediction will fall within that range.\n",
    "\n",
    "The key difference from confidence intervals is how they’re interpreted. Confidence intervals are about long-term averages—how often the interval would contain the true value if you repeated the experiment a bunch of times. Credible intervals, on the other hand, are much more straightforward and interpretable: they tell you the probability of the prediction being in that range for the case you’re looking at right now.\n",
    "\n",
    "The credible intervals are obtained using the model's `predict_interval` method, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_credible_interval = model.predict_interval(X_test_with_ones, coverage=0.8)\n",
    "style_data(predictive_credible_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "### Point predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53",
   "metadata": {},
   "source": [
    "To obtain point predictions, we can use the `predict` method instead. This will return the median of the posterior predictive distributions described above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = model.predict(X_test_with_ones)\n",
    "style_data(y_test_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55",
   "metadata": {},
   "source": [
    "## Advantages \n",
    "\n",
    "The main advantage of the conjugate prior frameworks such as the one we saw above is their **analytical simplicity**. When a conjugate prior is used, the posterior distribution can be found analytically. This eliminates the need for computationally intensive methods such as MCMC. This simplicity and the presence of closed form solution also extend to the derivation of posterior predictive distributions.\n",
    "\n",
    "Another key benefit is **computational efficiency**. The availability of a closed-form solution for the posterior distribution makes the process fast. \n",
    "\n",
    "Finally, conjugate priors enhance **interpretability**. Since the prior, likelihood, and posterior share the same functional form, it is easier to see how prior beliefs combine with observed data to produce the posterior distribution. This transparency allows for deeper insights into how the data and prior assumptions influence the final results.\n",
    "\n",
    "## Disadvantages\n",
    "\n",
    "Conjugate priors suffer from **limited flexibility**. They impose constraints on the form of the prior distribution, and the chosen prior may not accurately reflect true prior knowledge about the parameters. To reiterate: conjugate priors only work when both the likelihood and prior belong to the same distributio family. Thus, if the likelihood and prior do not align, the conjugate strategy cannot be applied.\n",
    "\n",
    "Additionally, conjugate priors are **inadequate for complex models**. Modern Bayesian models, such as neural networks or probabilistic graphical models, frequently require non-conjugate priors to capture intricate relationships between variables. In such cases, other inference techniques like MCMC or Variational Inference (VI) are necessary to approximate the posterior.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "While conjugate priors excel in providing analytical tractability and computational efficiency, they are inherently rigid. They are most effective for simple, structured models or situations where interpretability and fast computations are prioritized. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {
    "id": "cNKM7EPXJXij"
   },
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {
    "id": "KzuX_hc8vsD7"
   },
   "source": [
    "- [Bishop - Pattern Recognition and Machine Learning (2006)](https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf) - A comprehensive reference on machine learning theory and Bayesian methods by Christopher M. Bishop."
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "pymc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
