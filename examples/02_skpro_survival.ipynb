{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## time-to-event modelling and survival prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-up instructions:** On binder, this should run out-of-the-box.\n",
    "\n",
    "To run locally instead, ensure that `skpro` with basic dependency requirements is installed in your python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` provides a unified interface to time-to-event prediction models, also known as survival prediction models.\n",
    "\n",
    "**Time-to-event prediction** is a form of probabilistic regression where **labels can be \"censored\"**, i.e., of the form \"time is t or later\" instead of exat observations.\n",
    "\n",
    "**Section 1** provides an overview of the basic **time-to-event prediction workflows** supported by `skpro`.\n",
    "\n",
    "**Section 2** showcases **performance metrics and benchmarking** for time-to-event prediction with censored data.\n",
    "\n",
    "**Section 3** discusses **advanced composition patterns**, including various ways to leverage `sklearn` regressors for time-to-event prediction with censored data.\n",
    "\n",
    "**Section 4** gives an introduction to how to write **custom estimators** compliant with the `skpro` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hide warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic survival prediction interface <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section:\n",
    "\n",
    "* explanation of censored time-to-event data\n",
    "* `skpro` time-to-event/survival prediction interface\n",
    "* metrics, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 data representation, censoring"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Survival prediction or time-to-event prediction can be seen a generalization of probabilistic supervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Each sample consists of:\n",
    "\n",
    "* a feature vector, row of a data frame\n",
    "* a label, which can be an exact time of occurrence, or a statement about \"time was t or later\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulated toy datset, lung cancer survival times\n",
    "import numpy as np\n",
    "\n",
    "# demographics - age and smoker yes/no\n",
    "age = np.random.uniform(low=20, high=100, size=50)\n",
    "smoker = np.random.binomial(1, 0.3, size=50)\n",
    "\n",
    "# actual survival time\n",
    "scale = 200 / (0.5 * age + 30 * smoker)\n",
    "survival = scale * np.random.weibull(1, size=50)\n",
    "\n",
    "# patients are observed only for 5 years\n",
    "# if they surviva 5 years, we know they survived 5 years, but not exact time of death\n",
    "censored = survival > 5\n",
    "observation = np.minimum(survival, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` represents this information in an `sklearn`-like interface:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# features\n",
    "X = pd.DataFrame({\"age\": age, \"smoker\": smoker})\n",
    "\n",
    "# time of survival or censoring\n",
    "y = pd.DataFrame({\"time\": observation})\n",
    "\n",
    "# indicator whether event was observed or censored\n",
    "# censored = 1/True, observed = 0/False\n",
    "# variable names should be the same as for y\n",
    "C = pd.DataFrame({\"time\": censored})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 basic survival prediction workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "survival prediction is the task:\n",
    "\n",
    "* given censored time-to-event labels and features, `X`, `y`, `C`\n",
    "* learn a model that can predict `y` if it were uncensored, i.e., the true event time\n",
    "* the prediction should take the form of a survival function or probability distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` survival predictors extend the interface of probabilistic regressors:\n",
    "\n",
    "* `fit(X, y, C=None)`, with `X`, `y`, `C` as above; if `C=None`, all observations are uncensored\n",
    "* `predict(X_test)` for mean survival time predictions\n",
    "* `predict_proba(X_test)` for distributional predictions\n",
    "\n",
    "Other prediction methods - `predict_interval`, `predict_quantiles`, `predict_var` - also generalize the same way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because `C` is optional, and means \"uncensored\" if not passed, all survival prediction models can be used as supervised probabilistic regressors.\n",
    "\n",
    "Using probabilistic regressors as survival models is similarly possible, to be revisited later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic deployment workflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skpro.survival.coxph import CoxPH\n",
    "\n",
    "# step 1: data specification\n",
    "# X, y, C, as above\n",
    "X_train, X_new, y_train, _, C_train, _ = train_test_split(X, y, C)\n",
    "\n",
    "# step 2: specifying the regressor\n",
    "# example - Cox proportional hazards model from statsmodels\n",
    "surv_model_cox = CoxPH()\n",
    "\n",
    "# step 3: fitting the model to training data\n",
    "surv_model_cox.fit(X_train, y_train, C_train)\n",
    "\n",
    "# step 4: predicting labels on new data\n",
    "\n",
    "# full distribution prediction\n",
    "y_pred_proba_cox = surv_model_cox.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean predicted survival time\n",
    "y_pred_proba_cox.mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of survival functions\n",
    "y_pred_proba_cox.iloc[range(5)].plot(\"surv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting survival funtions in one figure, smokers in red\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "_, ax = subplots()\n",
    "\n",
    "for i in range(len(y_pred_proba_cox)):\n",
    "    ax = y_pred_proba_cox.iat[i, 0].plot(\"surv\", ax=ax, color=[\"b\", \"r\"][smoker[i]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 survival prediction with parametric predictive distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example: using an accelerated failure time model with Weibull hazard\n",
    "\n",
    "same workflow, only using different model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skpro.survival.aft import AFTWeibull\n",
    "\n",
    "# step 1: data specification\n",
    "# X, y, C, as above\n",
    "X_train, X_new, y_train, _, C_train, _ = train_test_split(X, y, C)\n",
    "\n",
    "# step 2: specifying the regressor\n",
    "# example - Cox proportional hazards model from statsmodels\n",
    "surv_model_aft = AFTWeibull()\n",
    "\n",
    "# step 3: fitting the model to training data\n",
    "surv_model_aft.fit(X_train, y_train, C_train)\n",
    "\n",
    "# step 4: predicting labels on new data\n",
    "\n",
    "# full distribution prediction\n",
    "y_pred_proba_aft = surv_model_aft.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting survival funtions in one figure, smokers in red\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "_, ax = subplots()\n",
    "\n",
    "for i in range(len(y_pred_proba_cox)):\n",
    "    ax = y_pred_proba_aft.iat[i, 0].plot(\n",
    "        \"surv\", ax=ax, color=[\"b\", \"r\"][smoker[i]], x_bounds=[0, 5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hazard functions can be plotted the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot of hazard functions\n",
    "y_pred_proba_aft.iloc[range(5)].plot(\"haz\", x_bounds=[0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting survival funtions in one figure, smokers in red\n",
    "from matplotlib.pyplot import subplots\n",
    "\n",
    "_, ax = subplots()\n",
    "\n",
    "for i in range(len(y_pred_proba_aft)):\n",
    "    ax = y_pred_proba_aft.iat[i, 0].plot(\n",
    "        \"haz\", ax=ax, color=[\"b\", \"r\"][smoker[i]], x_bounds=[0, 5]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estimated scale parameter\n",
    "y_pred_proba_aft.to_df().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# actual Weibull scale parameter to compare\n",
    "# unknown in a real scenario, but we know since we simulated the data\n",
    "scale[0:5]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 simple evaluation workflow for time-to-event predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for simple evaluation:\n",
    "\n",
    "1. split the data into train/test set - including the censoring variable\n",
    "2. make predictions of either type for test features\n",
    "3. compute metric on test set, comparing test predictions to held out test observations,\n",
    "  including censoring indicsator\n",
    "\n",
    "Note:\n",
    "\n",
    "* metrics will compare probabilistic prediction to tabular ground truth and\n",
    "  censoring indicator\n",
    "* the metric will needs to be of a compatible type, e.g., for proba predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skpro.metrics import ConcordanceHarrell\n",
    "from skpro.survival.coxph import CoxPH\n",
    "\n",
    "# step 1: data specification\n",
    "X_train, X_test, y_train, y_test, C_train, C_test = train_test_split(X, y, C)\n",
    "\n",
    "# step 2: specifying the regressor\n",
    "# example - Cox proportional hazards model from statsmodels\n",
    "surv_model = CoxPH()\n",
    "\n",
    "# step 3: fitting the model to training data\n",
    "surv_model.fit(X_train, y_train, C_train)\n",
    "\n",
    "# step 4: predicting labels on new data\n",
    "y_pred_proba = surv_model.predict_proba(X_test)\n",
    "\n",
    "# step 5: specifying evaluation metric\n",
    "metric = ConcordanceHarrell()\n",
    "\n",
    "# step 6: evaluate metric, compare predictions to actuals\n",
    "metric(y_test, y_pred_proba, C_true=C_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we know that metric is of right type? Via `scitype:y_pred` tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.get_tags()\n",
    "# scitype:y_pred is pred_proba - for proba predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we find metrics for a prediction type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "all_objects(\"metric\", as_dataframe=True, return_tags=\"scitype:y_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra note: quantile metrics can be applied to interval predictions as well\n",
    "\n",
    "more details on metrics below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 `skpro` objects - `scikit-base` interface, searching for regressors and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `skpro` objects - `skbase` interface points `get_tags`, `get_params`/`set_params`\n",
    "* searching estimators and metrics via `all_objects`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.1 primer on `skpro` object interface <a class=\"anchor\" id=\"section1_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics and estimators are first-class citizens in `skpro`, with a `scikit-base` compatible interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example object 1: CRPS metric\n",
    "from skpro.metrics import CRPS\n",
    "\n",
    "crps_metric = CRPS()\n",
    "\n",
    "# example object 2: ResidualDouble regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "reg_mean = LinearRegression()\n",
    "reg_resid = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean, reg_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g., all have `get_tags` interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_metric.get_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_proba.get_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the tag `object_type` indicates the type of object, e.g., metric or proba regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all objects also have the `get_params`/`set_params` interface known from `scikit-learn`\n",
    "\n",
    "= reading or setting hyper-parameters\n",
    "\n",
    "`get_params` returns `dict` `{paramname: paramvalue}`; `set_params` writes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_metric.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "composite objects have the nested param interface, keys `componentname__paramname`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that reg_proba has components LinearRegression and RandomForestaregressor\n",
    "# each with their own parameters\n",
    "reg_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so `reg_proba` will have parameters coming from itself and either component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_proba.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further common interface points are `get_config`, `set_config`, and `get_fitted_params` (only fittable estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4.2 searching for regressors and metrics <a class=\"anchor\" id=\"section1_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as first-class citizens, all objects in `skpro` are indexed via the `registry` utility `all_objects`.\n",
    "\n",
    "To find probabilistic supervised regressors, use `all_objects` with the type `regressor_proba`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "all_objects(\"regressor_proba\", as_dataframe=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a full list can also be found in the online API reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for metrics, as seen above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "all_objects(\"metric\", as_dataframe=True, return_tags=\"scitype:y_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all tags can be printed by the `all_tags` utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tags applicable to metrics\n",
    "from skpro.registry import all_tags\n",
    "\n",
    "all_tags(\"metric\", as_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tags applicable to probabilistic regressors\n",
    "from skpro.registry import all_tags\n",
    "\n",
    "all_tags(\"regressor_proba\", as_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering in search can be done with the `filter_tags` argument in `all_objects`, see docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "# \"retrieve all genuinely probabilistic loss functions\"\n",
    "all_objects(\"metric\", as_dataframe=True, filter_tags={\"scitype:y_pred\": \"pred_proba\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prediction types, metrics, benchmarking <a class=\"anchor\" id=\"chapter2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section gives more details on:\n",
    "\n",
    "* different prediction types, including a methodological primer\n",
    "* the API of metrics to compare probabilistic predictions to non-probabilistic actuals\n",
    "* utilities for batch benchmarking of estimators and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Probabilistic predictions - methodological primer <a class=\"anchor\" id=\"section2_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**readers familir with, or less interested in theory, may like to skip section 2.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In supervised learning - probabilistic or not:\n",
    "\n",
    "* we fit estimator to i.i.d samples $(X_1, Y_1), \\dots, (X_N, Y_N) \\sim (X_*, Y_*)$\n",
    "* and want to predict $y$ given $x$ accurately, for $(x, y) \\sim (X_*, Y_*)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $y$ be the (true) value, for an observed feature $x$\n",
    "\n",
    "(we consider $y$ a random variable)\n",
    "\n",
    "| Name | param | prediction/estimate of | `skpro` |\n",
    "| ---- | ----- | ---------------------- | -------- |\n",
    "| point prediction | | conditional expectation $\\mathbb{E}[y\\|x]$ | `predict` |\n",
    "| variance prediction | | conditional variance $Var[y\\|x]$ | `predict_var` |\n",
    "| quantile prediction | $\\alpha\\in (0,1)$ | $\\alpha$-quantile of $y\\|x$ | `predict_quantiles` |\n",
    "| interval prediction | $c\\in (0,1)$| $[a,b]$ s.t. $P(a\\le y \\le b\\| x) = c$ | `predict_interval` |\n",
    "| distribution prediction | | the law/distribution of $y\\|x$ | `predict_proba` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More formal details & intuition:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's consider the toy example again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_new, y_train, _ = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_new, y_train, _ = train_test_split(X, y)\n",
    "\n",
    "\n",
    "reg_mean = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean)\n",
    "\n",
    "reg_proba.fit(X_train, y_train)\n",
    "y_pred_proba = reg_proba.predict_proba(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a **\"point prediction\"** is a prediction/estimate of the conditional expectation $\\mathbb{E}[y|x]$.\\\n",
    " **Intuition**: \"out of many repetitions/worlds, this value is the arithmetic average of all observations\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if y_pred_proba were *true*, here's how many repetitions would look like:\n",
    "\n",
    "# repeating this line is \"one repetition\"\n",
    "y_pred_proba.sample().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "many_samples = y_pred_proba.sample(100)\n",
    "many_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"doing many times and taking the mean\" -> usual point prediction\n",
    "mean_prediction = many_samples.groupby(level=1, sort=False).mean()\n",
    "mean_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we would do this infinity times instead of 100:\n",
    "y_pred_proba.mean().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a **\"variance prediction\"** is a prediction/estimate of the conditional expectation $Var[y|x]$.\\\n",
    " **Intuition:** \"out of many repetitions/worlds, this value is the average squared distance of the observation to the perfect point prediction\".\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same as above - take many samples, and then compute element-wise statistics\n",
    "var_prediction = many_samples.groupby(level=1, sort=False).var()\n",
    "var_prediction.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e.g., predict_var should give the same result as infinite large sample's variance\n",
    "y_pred_proba.var().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a **\"quantile prediction\"**, at quantile point $\\alpha\\in (0,1)$ is a prediction/estimate of the $\\alpha$-quantile of $y'|y$, i.e., of $F^{-1}_{y|x}(\\alpha)$, where $F^{-1}$ is the (generalized) inverse cdf = quantile function of the random variable y|x.\\\n",
    " **Intuition**: \"out of many repetitions/worlds, a fraction of exactly $\\alpha$ will have equal or smaller than this value.\"\n",
    "* an **\"interval prediction\"** or \"predictive interval\" with (symmetric) coverage $c\\in (0,1)$ is a prediction/estimate pair of lower bound $a$ and upper bound $b$ such that $P(a\\le y \\le b| x) = c$ and $P(y \\gneq b| x) = P(y \\lneq a| x) = (1 - c) /2$.\\\n",
    " **Intuition**: \"out of many repetitions/worlds, a fraction of exactly $c$ will be contained in the interval $[a,b]$, and being above is equally likely as being below\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(similar - exercise left to the reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* a **\"distribution prediction\"** or \"full probabilistic prediction\" is a prediction/estimate of the distribution of $y|x$, e.g., \"it's a normal distribution with mean 42 and variance 1\".\\\n",
    "**Intuition**: exhaustive description of the generating mechanism of many repetitions/worlds."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "note: the true distribution is unknown, and not accessible easily!\n",
    "\n",
    "`y_pred_proba` is a distribution, but in general not equal to the true one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "that is, there are:\n",
    "\n",
    "* *true* distribution `y_pred_proba_true` - unknown and unknowable but estimable\n",
    "* `y_pred_proba` - our guess at `y_pred_proba_true`\n",
    "* the actual data `y_true` is *one* `y_pred_proba_true.sample()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `predict` produces guess of `y_pred_proba_true.mean()`\n",
    "* `predict_var` produces guess of `y_pred_proba_true.var()`\n",
    "* `predict_quantiles([0.05, 0.5, 0.95])` produces guess of `y_pred_proba_true.quantiles([0.05, 0.5, 0.95])`\n",
    "* `predict_proba` produces guess of `y_pred_proba_true`\n",
    "\n",
    "the guesses are algorithm specific, and some algorithms are more accurate than others, given data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 probabilistic metrics - details <a class=\"anchor\" id=\"section2_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "General usage pattern same as for `sklearn` metrics:\n",
    "\n",
    "1. get some actuals and predictions\n",
    "2. specify the metric - similar to estimator specs\n",
    "3. plug the actuals and predictions into metric to get metric values\n",
    "\n",
    "*but*: need to use dedicated metric for probabilistic predictions\n",
    "\n",
    "* ground truth: `y_true` samples\n",
    "* prediction e.g., `y_predict_proba`, `y_predict_interval`\n",
    "* so, match metric with type of prediction!\n",
    "    * `metric(y_true: 2D pd.DataFrame, y_pred: proba_prediction_type) -> float`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall methods available for all probabilistic regressors:\n",
    "\n",
    "- `predict_interval` produces interval predictions.\n",
    "  Argument `coverage` (nominal interval coverage) must be provided.\n",
    "- `predict_quantiles` produces quantile predictions.\n",
    "  Argument `alpha` (quantile values) must be provided.\n",
    "- `predict_var` produces variance predictions. Same args as `predict`.\n",
    "- `predict_proba` produces full distributional predictions. Same args as `predict`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Name | param | prediction/estimate of | `skpro` |\n",
    "| ---- | ----- | ---------------------- | -------- |\n",
    "| point prediction | | conditional expectation $\\mathbb{E}[y\\|x]$ | `predict` |\n",
    "| variance prediction | | conditional variance $Var[y\\|x]$ | `predict_var` |\n",
    "| quantile prediction | $\\alpha\\in (0,1)$ | $\\alpha$-quantile of $y\\|x$ | `predict_quantiles` |\n",
    "| interval prediction | $c\\in (0,1)$| $[a,b]$ s.t. $P(a\\le y \\le b\\| x) = c$ | `predict_interval` |\n",
    "| distribution prediction | | the law/distribution of $y\\|x$ | `predict_proba` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's produce some probabilistic predictions!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. get some actuals and predictions\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "# actuals = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "reg_mean = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean)\n",
    "\n",
    "reg_proba.fit(X_train, y_train)\n",
    "\n",
    "# use any of the probabilistic methods, we have seen this\n",
    "y_pred_int = reg_proba.predict_interval(X_test, coverage=0.95)\n",
    "y_pred_q = reg_proba.predict_quantiles(X_test, alpha=[0.05, 0.95])\n",
    "y_pred_proba = reg_proba.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall, all have their own output format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_int  # lower/upper intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_q  # quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba  # sktime/skpro BaseDistribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we now need to apply a suitable metric, `metric(y_test, y_pred)`\n",
    "\n",
    "IMPORTANT: sequence matters, `y_test` first; `y_pred` has very different type!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. specify metric\n",
    "# CRPS = continuous ranked probability score, for distribution predictions\n",
    "from skpro.metrics import CRPS\n",
    "\n",
    "crps = CRPS()\n",
    "\n",
    "# 3. evaluate metric\n",
    "crps(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we find a metric that fits the prediction type?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "answer: metrics are tagged\n",
    "\n",
    "important tag: `scitype:y_pred`\n",
    "\n",
    "* `\"pred_proba\"` - distributional, can applied to distributions, `predict_proba` output\n",
    "* `\"pred_quantiles\"` - quantile forecast metric, can be applied to quantile predictions, interval predictions, distributional predictions\n",
    "    * applicable to `predict_quantiles`, `predict_interval`, `predict_proba` outputs\n",
    "* `\"pred_interval\"` - interval forecast metric, can be applied to interval predictions, distributional predictions\n",
    "    * applicable to `predict_interval`, `predict_proba` outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps.get_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "listing metrics with the tag, filtering for probabilistic tags:\n",
    "\n",
    "(let's try to find a quantile prediction metric!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "all_objects(\n",
    "    \"metric\",\n",
    "    as_dataframe=True,\n",
    "    return_tags=\"scitype:y_pred\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`PinballLoss` is a quantile forecast metric:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.metrics import PinballLoss\n",
    "\n",
    "pinball_loss = PinballLoss()\n",
    "\n",
    "pinball_loss(y_test, y_pred_q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... this is by default an average (grand average, float)\n",
    "\n",
    "* averages over samples in `y_pred` / `y_test` (rows)\n",
    "* averages over variables (columns)\n",
    "* average over `alpha` values, quantile points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "what if we don't want these averages?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* variable (column) averaging is controlled by the `multioutput` arg.\n",
    "    * `\"raw_values\"` prevents averaging, `\"uniform_average\"` computes arithmetic mean.\n",
    "* quantile points (`alpha`) or coverage (`coverage`) is controlled by `score_average` arg\n",
    "* evaluation by row via the `evaluate_by_index` method\n",
    "    * can be useful for diagnostics or statistical tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 1: Pinball loss by quantile point\n",
    "loss_multi = PinballLoss(score_average=False)\n",
    "loss_multi(y_test, y_pred_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 2: CRPS by test sample index\n",
    "crps.evaluate_by_index(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caveat: not every metric is an average over time points, e.g., RMSE\n",
    "\n",
    "In this case, `evaluate_by_index` computes jackknife pseudo-samples\n",
    "\n",
    "(for mean statistics, jackknife pseudo-samples are equal to individual samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Benchmark evaluation of probabilistic regressors <a class=\"anchor\" id=\"section2_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for quick evaluation and benchmarking,\n",
    "\n",
    "the `benchmarking.evaluate` utility can be used:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from skpro.benchmarking.evaluate import evaluate\n",
    "from skpro.metrics import CRPS\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "# 1. specify dataset\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "\n",
    "# 2. specify estimator\n",
    "estimator = ResidualDouble(LinearRegression())\n",
    "\n",
    "# 3. specify cross-validation schema\n",
    "cv = KFold(n_splits=3)\n",
    "\n",
    "# 4. specify evaluation metric\n",
    "crps = CRPS()\n",
    "\n",
    "# 5. evaluate - run the benchmark\n",
    "results = evaluate(estimator=estimator, X=X, y=y, cv=cv, scoring=crps)\n",
    "\n",
    "# results are pd.DataFrame\n",
    "# each row is one repetition of the cross-validation on one fold fit/predict/evaluate\n",
    "# columns report performance, runtime, and other optional information (see docstring)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced composition patterns <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we introduce a number of composition patterns available in `skpro`:\n",
    "\n",
    "* reducer-wrappers that turn `sklearn` regressors into probabilistic ones\n",
    "* pipelines of `sklearn` transformers with `skpro` regressors\n",
    "* tuning `skpro` probabilistic regressors via grid/random search, minimizing a probabilistic metric\n",
    "* ensembling multiple `skpro` probabilistic regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data used in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation metric used in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps = CRPS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Reducers to turn `sklearn` regressors probabilistic <a class=\"anchor\" id=\"section3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are many common algorithms that turn a non-probabilistic tabular regressor probabilistic\n",
    "\n",
    "formally, this is a type of \"reduction\" - of probabilistic supervised tabular to non-probabilistic supervised tabular\n",
    "\n",
    "Examples:\n",
    "\n",
    "* predicting variance equal to training residual variance - `ResidualDouble` with standard settings\n",
    "    * or other unconditional distribution estimate for residuals\n",
    "* \"squaring the residual\" two-step prediction - `ResidualDouble`\n",
    "* boostrap prediction intervals - `BootstrapRegressor`\n",
    "* conformal prediction intervals - contributions appreciated :-)\n",
    "* natural gradient boosting aka NGBoost - contributions appreciated :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 constant variance prediction <a class=\"anchor\" id=\"section3_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# estimator specification - use any sklearn regressor for reg_mean\n",
    "reg_mean = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean, cv=KFold(5))\n",
    "# cv is used to estimate out-of-sample residual variance via 5-fold CV\n",
    "# note - in-sample predictions will usually underestimate the variance!\n",
    "\n",
    "# fit and predict\n",
    "reg_proba.fit(X_train, y_train)\n",
    "y_pred_proba = reg_proba.predict_proba(X_test)\n",
    "\n",
    "# evaluate\n",
    "crps(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_interval\n",
    "\n",
    "plot_crossplot_interval(y_test, y_pred_proba, coverage=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 two-step residual prediction <a class=\"anchor\" id=\"section3_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# estimator specification - use any sklearn regressor for reg_mean and reg_resid\n",
    "reg_mean = RandomForestRegressor()\n",
    "reg_resid = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean, estimator_resid=reg_resid, cv=KFold(5))\n",
    "# cv is used to estimate out-of-sample residual variance via 5-fold CV\n",
    "\n",
    "# fit and predict\n",
    "reg_proba.fit(X_train, y_train)\n",
    "y_pred_proba = reg_proba.predict_proba(X_test)\n",
    "\n",
    "# evaluate\n",
    "crps(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_interval\n",
    "\n",
    "plot_crossplot_interval(y_test, y_pred_proba, coverage=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 bootstrap prediction intervals <a class=\"anchor\" id=\"section3_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from skpro.regression.bootstrap import BootstrapRegressor\n",
    "\n",
    "# estimator specification - use any sklearn regressor for reg_mean\n",
    "reg_mean = LinearRegression()\n",
    "reg_proba = BootstrapRegressor(reg_mean, n_bootstrap_samples=100)\n",
    "\n",
    "# fit and predict\n",
    "reg_proba.fit(X_train, y_train)\n",
    "y_pred_proba = reg_proba.predict_proba(X_test)\n",
    "\n",
    "# evaluate\n",
    "crps(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_interval\n",
    "\n",
    "plot_crossplot_interval(y_test, y_pred_proba, coverage=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pipelines of `skpro` regressor and `sklearn` transformers <a class=\"anchor\" id=\"section3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` regressors can be pipelined with `sklearn` transformers, using the `skpro` pipeline.\n",
    "\n",
    "This ensure presence of `predict_proba` etc in the pipeline object.\n",
    "\n",
    "The syntax is exactly the same as for `sklearn`'s pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from skpro.regression.compose import Pipeline\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "# estimator specification\n",
    "reg_mean = LinearRegression()\n",
    "reg_proba = ResidualDouble(reg_mean)\n",
    "\n",
    "# pipeline is specified as a list of tuples (name, estimator)\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", Imputer()),  # an sklearn transformer\n",
    "        (\"scaler\", MinMaxScaler()),  # an sklearn transformer\n",
    "        (\"regressor\", reg_proba),  # an skpro regressor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pipeline behaves as any skpro regressor\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X=X_test)\n",
    "y_pred_proba = pipe.predict_proba(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the pipeline provides the familiar nested `get_params`, `set_params` interface:\n",
    "\n",
    "nested parameters are keyed `componentname__parametername`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipelines can also be created via simple lists of estimators,\n",
    "\n",
    "in this case names are generated automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline is specified as a list of tuples (name, estimator)\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        Imputer(),  # an sklearn transformer\n",
    "        MinMaxScaler(),  # an sklearn transformer\n",
    "        reg_proba,  # an skpro regressor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Tuning of `skpro` regressors via grid and random search <a class=\"anchor\" id=\"section3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` provides grid and random search tuners to tune arbitrary probabilistic regressors,\n",
    "\n",
    "using probabilistic metrics. Besides this, they function as the `sklearn` tuners do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from skpro.metrics import CRPS\n",
    "from skpro.model_selection import GridSearchCV\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "# cross-validation specification for tuner\n",
    "cv = KFold(n_splits=3)\n",
    "\n",
    "# estimator to be tuned\n",
    "estimator = ResidualDouble(LinearRegression())\n",
    "\n",
    "# tuning grid - do we fit an intercept in the linear regression?\n",
    "param_grid = {\"estimator__fit_intercept\": [True, False]}\n",
    "\n",
    "# metric to be optimized\n",
    "crps_metric = CRPS()\n",
    "\n",
    "# specification of the grid search tuner\n",
    "gscv = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=crps_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the grid search tuner behaves like any `skpro` probabilistic regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.fit(X_train, y_train)\n",
    "y_pred = gscv.predict(X_test)\n",
    "y_pred_proba = gscv.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random search is similar, except that instead of a grid a parameter sampler should be specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.model_selection import RandomizedSearchCV\n",
    "\n",
    "# only difference to GridSearchCV is the param_distributions argument\n",
    "\n",
    "# specification of the random search parameter sampler\n",
    "param_distributions = {\"estimator__fit_intercept\": [True, False]}\n",
    "\n",
    "# specification of the random search tuner\n",
    "rscv = RandomizedSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_distributions=param_distributions,\n",
    "    cv=cv,\n",
    "    scoring=crps_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 Bagging/mixture ensemble of probabilistic regressors <a class=\"anchor\" id=\"section3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Classical bagging does the following, for a wrapped estimator:\n",
    "\n",
    "In `fit`:\n",
    "\n",
    "1. subsample rows and/or columns of `X`, `y` to `X_subs`, `y_subs`\n",
    "2. fit clone of wrapped estimator to `X_subs`, `y_subs`\n",
    "3. Repeat 1-2 `n_estimators` times, store that many fitted clones.\n",
    "\n",
    "In `predict`, for `X_test`:\n",
    "\n",
    "1. for all fitted clones, obtain predictions on `X_test` - these are distributions\n",
    "2. return the uniform mixture of these distributions, per test sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from skpro.regression.ensemble import BaggingRegressor\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "reg_mean = LinearRegression()\n",
    "reg_proba = ResidualDouble(reg_mean)\n",
    "\n",
    "ens = BaggingRegressor(reg_proba, n_estimators=10)\n",
    "ens.fit(X_train, y_train)\n",
    "\n",
    "y_pred = ens.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred is a mixture distribution!\n",
    "str(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[type(x) for x in y_pred.distributions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Extension guide - implementing your own probabilistic regressor <a class=\"anchor\" id=\"chapter4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "`skpro` is meant to be easily extensible, for direct contribution to `skpro` as well as for local/private extension with custom methods.\n",
    "\n",
    "To get started:\n",
    "\n",
    "* Follow the [\"implementing estimator\" developer guide](https://skpro.readthedocs.io/en/stable/developer_guide/add_estimators.html)\n",
    "* Use the [probabilistic regressor template](https://github.com/sktime/skpro/blob/main/extension_templates/regression.py) to get started\n",
    "\n",
    "1. Read through the [probabilistic regression extension template](https://github.com/sktime/skpro/blob/main/extension_templates/regression.py) - this is a `python` file with `todo` blocks that mark the places in which changes need to be added.\n",
    "2. Copy the proba regressor extension template to a local folder in your own repository (local/private extension), or to a suitable location in your clone of the `skpro` or affiliated repository (if contributed extension), inside `skpro.regression`; rename the file and update the file docstring appropriately.\n",
    "3. Address the \"todo\" parts. Usually, this means: changing the name of the class, setting the tag values, specifying hyper-parameters, filling in `__init__`, `_fit`, and at least one of the probabilistic prediction methods, preferably `_predict_proba` (for details see the extension template). You can add private methods as long as they do not override the default public interface. For more details, see the extension template.\n",
    "4. To test your estimator manually: import your estimator and run it in the worfklows in Section 1; then use it in the compositors in Section 3.\n",
    "5. To test your estimator automatically: call `skpro.utils.check_estimator` on your estimator. You can call this on a class or object instance. Ensure you have specified test parameters in the `get_test_params` method, according to the extension template.\n",
    "\n",
    "In case of direct contribution to `skpro` or one of its affiliated packages, additionally:\n",
    "\n",
    "* Add yourself as an author to the code, and to the `CODEOWNERS` for the new estimator file(s).\n",
    "* Create a pull request that contains only the new estimators (and their inheritance tree, if it's not just one class), as well as the automated tests as described above.\n",
    "* In the pull request, describe the estimator and optimally provide a publication or other technical reference for the strategy it implements.\n",
    "* Before making the pull request, ensure that you have all necessary permissions to contribute the code to a permissive license (BSD-3) open source project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary<a class=\"anchor\" id=\"chapter5\"></a>\n",
    "\n",
    "* `skpro` is a unified interface toolbox for probabilistic supervised regression, that is, for prediction intervals, quantiles, fully distributional predictions, in a tabular regression setting. The interface is fully interoperable with `scikit-learn` and `scikit-base` interface specifications.\n",
    "\n",
    "* `skpro` comes with rich composition functionality that allows to build complex pipelines easily, and connect easily with other parts of the open source ecosystem, such as `scikit-learn` and individual algorithm libraries.\n",
    "\n",
    "* `skpro` is easy to extend, and comes with user friendly tools to facilitate implementing and testing your own probabilistic regressors and composition principles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Credits:\n",
    "\n",
    "noteook creation: fkiraly\n",
    "\n",
    "skpro: https://github.com/sktime/skpro/blob/main/CONTRIBUTORS.md"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e631b8a076cc106144e9b132b7d31cae2f1e2660b47e5f9fcb0397caae5fbd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
