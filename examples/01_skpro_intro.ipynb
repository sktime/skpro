{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## skpro introduction notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Set-up instructions:** On binder, this should run out-of-the-box.\n",
    "\n",
    "To run this notebook as intended, ensure that `skpro` with basic dependency requirements is installed in your python environment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` provides `scikit-learn`-like, `scikit-base` compatible interfaces to:\n",
    "\n",
    "* tabular **supervised regressors with probabilistic prediction modes** - interval, quantile and distribution predictions\n",
    "* **performance metrics to evaluate probabilistic predictions**, e.g., pinball loss, empirical coverage, CRPS\n",
    "* **reductions** to turn non-probabilistic, `scikit-learn` regressors into probabilistic `skpro` regressors, such as bootstrap or conformal\n",
    "* tools for building **pipelines and composite machine learning models**, including tuning via probabilistic performance metrics\n",
    "* symbolic an lazy **probability distributions** with a value domain of `pandas.DataFrame`-s and a `pandas`-like interface\n",
    "\n",
    "**Section 1** provides an overview of common **probabilistic supervised regression workflows** supported by `skpro`.\n",
    "\n",
    "**Section 2** gives an more detailed introduction to **prediction modes, performance metrics, and benchmarking tools**.\n",
    "\n",
    "**Section 3** discusses **advanced composition patterns**, including various ways to add probabilistic capability to any `sklearn` regressor, pipeline building, tuning, ensembling.\n",
    "\n",
    "**Section 4** gives an introduction to how to write **custom estimators** compliant with the `skpro` interface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "\n",
    "# hide warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Basic probabilistic supervised regression workflows <a class=\"anchor\" id=\"chapter1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` revolves around supervised probabilistic regressors:\n",
    "\n",
    "* `fit(X, y)` with tabular features `X`, labels `y`, same rows, both `pd.DataFrame`\n",
    "* `predict_interval(X_test)` for interval predictions of labels\n",
    "* `predict_quantiles(X_test)` for quantile predictions of labels\n",
    "* `predict_var(X_test)` for variance predictions of labels\n",
    "* `predict(X_test)` for mean predictions\n",
    "* `predict_proba(X_test)` for distributional prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 basic deployment workflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` regressors are used via `fit` then `predict_proba` etc.\n",
    "\n",
    "Same as `sklearn` regressors - `X` and `y` should be `pd.DataFrame` (`numpy` is also ok but not recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "# step 1: data specification\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_new, y_train, _ = train_test_split(X, y)\n",
    "\n",
    "# step 2: specifying the regressor\n",
    "# example - random forest for mean prediction\n",
    "# linear regression for variance prediction\n",
    "reg_mean = RandomForestRegressor()\n",
    "reg_resid = LinearRegression()\n",
    "reg_proba = ResidualDouble(reg_mean, reg_resid)\n",
    "\n",
    "# step 3: fitting the model to training data\n",
    "reg_proba.fit(X_train, y_train)\n",
    "\n",
    "# step 4: predicting labels on new data\n",
    "\n",
    "# probabilistic prediction modes - pick any or multiple\n",
    "# we show the return types in detail below\n",
    "\n",
    "# full distribution prediction\n",
    "y_pred_proba = reg_proba.predict_proba(X_new)\n",
    "\n",
    "# interval prediction\n",
    "y_pred_interval = reg_proba.predict_interval(X_new, coverage=0.9)\n",
    "\n",
    "# quantile prediction\n",
    "y_pred_quantiles = reg_proba.predict_quantiles(X_new, alpha=[0.05, 0.5, 0.95])\n",
    "\n",
    "# variance prediction\n",
    "y_pred_var = reg_proba.predict_var(X_new)\n",
    "\n",
    "# mean prediction is same as \"classical\" sklearn predict, also available\n",
    "y_pred_mean = reg_proba.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.1 distribution predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`y_pred_proba` is an `skpro` distribution - it has index and columns like `pd.DataFrame`\n",
    "\n",
    "\"we predict that true labels are distributed according to `y_pred_proba`\"\n",
    "\n",
    "(here: distribution marginal by row/columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba = reg_proba.predict_proba(X_new)\n",
    "y_pred_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` distribution objects are pandas-like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba.index  # same index as X_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba.columns  # same columns as X_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "distribution objects have `sample` and methods such as `mean`, `var`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba.sample().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba.mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba.var().head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.2 interval predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "interval prediction `y_pred_interval` is a `pd.DataFrame`:\n",
    "\n",
    "* rows are the same as `X_new`\n",
    "* columns indicate variables, nominal coverage, and bottom/upper bound\n",
    "\n",
    "\"we predict that value in row falls between bottom/upper with 90% chance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_interval = reg_proba.predict_interval(X_new, coverage=0.9)\n",
    "y_pred_interval.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.3 quantile predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "quantile prediction `y_pred_quantiles` is a `pd.DataFrame`:\n",
    "\n",
    "* rows are the same as `X_new`\n",
    "* columns indicate variables, quantile points\n",
    "\n",
    "\"we predict the 5%, 50%, 95% quantile points for the row to be here\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_quantiles = reg_proba.predict_quantiles(X_new, alpha=[0.05, 0.5, 0.95])\n",
    "y_pred_quantiles.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1.4 mean and variance predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mean and variance predictions `y_pred_mean`, `y_pred_var` are `pd.DataFrame`-s:\n",
    "\n",
    "* rows are the same as `X_new`\n",
    "* columns are the same as `X_new`\n",
    "\n",
    "entries are predictive mean and variance in row/column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mean = reg_proba.predict(X_new)\n",
    "y_pred_var = reg_proba.predict_var(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_mean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_var.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this is the same as taking the distribution prediction and taking mean/variance\n",
    "\n",
    "(for distribution objects that estimate these precisely)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba.mean().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_proba.var().head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 simple evaluation workflow for probabilistic predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for simple evaluation:\n",
    "\n",
    "1. split the data into train/test set\n",
    "2. make predictions of either type for test features\n",
    "3. compute metric on test set, comparing test predictions to hend out test labels\n",
    "\n",
    "Note:\n",
    "\n",
    "* metrics will compare tabular ground truth to probabilistic prediction\n",
    "* the metric will needs to be of a compatible type, e.g., for proba predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skpro.metrics import CRPS\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "# step 1: data specification\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# step 2: specifying the regressor\n",
    "# example - linear regression for mean prediction\n",
    "# random forest for variance prediction\n",
    "reg_mean = LinearRegression()\n",
    "reg_resid = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean, reg_resid)\n",
    "\n",
    "# step 3: fitting the model to training data\n",
    "reg_proba.fit(X_train, y_train)\n",
    "\n",
    "# step 4: predicting labels on new data\n",
    "y_pred_proba = reg_proba.predict_proba(X_test)\n",
    "\n",
    "# step 5: specifying evaluation metric\n",
    "metric = CRPS()\n",
    "\n",
    "# step 6: evaluat metric, compare predictions to actuals\n",
    "metric(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we know that metric is of right type? Via `scitype:y_pred` tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric.get_tags()\n",
    "# scitype:y_pred is pred_proba - for proba predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how do we find metrics for a prediction type?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "all_objects(\"metric\", as_dataframe=True, return_tags=\"scitype:y_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "extra note: quantile metrics can be applied to interval predictions as well\n",
    "\n",
    "more details on metrics below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2.1 diagnostic visualisations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "some useful diagnostic visualisations: variants of crossplots for probabilistic predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A. crossplot ground truth vs prediction intervals.\n",
    "\n",
    "Works with both proba and interval predictions.\n",
    "\n",
    "What to look for: intervals shouhld cut through the x = y line (green points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_interval\n",
    "\n",
    "plot_crossplot_interval(y_test, y_pred_proba, coverage=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_interval\n",
    "\n",
    "y_pred_interval = reg_proba.predict_interval(X_test, coverage=0.9)\n",
    "plot_crossplot_interval(y_test, y_pred_interval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B. crossplot residuals vs predictive standard deviation\n",
    "\n",
    "Works with both proba and variance predictions.\n",
    "\n",
    "What to look for: should be close to a line, high linear correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_std\n",
    "\n",
    "plot_crossplot_std(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_std\n",
    "\n",
    "y_pred_var = reg_proba.predict_var(X_test)\n",
    "plot_crossplot_std(y_test, y_pred_var)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. crossplot ground truth vs loss values\n",
    "\n",
    "Loss and prediction type should agree.\n",
    "\n",
    "What to look for: association between accuracy and ground truth value\n",
    "\n",
    "Diagnostic of which values we can predict more accurately,\n",
    "\n",
    "e.g., to inform modelling or identify unusual outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_loss\n",
    "\n",
    "crps_metric = CRPS()\n",
    "plot_crossplot_loss(y_test, y_pred_proba, crps_metric)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 `skpro` objects - `scikit-base` interface, searching for regressors and metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.1 primer on `skpro` object interface <a class=\"anchor\" id=\"section1_3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "metrics and estimators are first-class citizens in `skpro`, with a `scikit-base` compatible interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example object 1: CRPS metric\n",
    "from skpro.metrics import CRPS\n",
    "\n",
    "crps_metric = CRPS()\n",
    "\n",
    "# example object 2: ResidualDouble regressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "reg_mean = LinearRegression()\n",
    "reg_resid = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean, reg_resid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e.g., all have `get_tags` interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_metric.get_tags()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_proba.get_tags()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the tag `object_type` indicates the type of object, e.g., metric or proba regressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all objects also have the `get_params`/`set_params` interface known from `scikit-learn`\n",
    "\n",
    "= reading or setting hyper-parameters\n",
    "\n",
    "`get_params` returns `dict` `{paramname: paramvalue}`; `set_params` writes it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps_metric.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "composite objects have the nested param interface, keys `componentname__paramname`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that reg_proba has components LinearRegression and RandomForestaregressor\n",
    "# each with their own parameters\n",
    "reg_proba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so `reg_proba` will have parameters coming from itself and either component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_proba.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "further common interface points are `get_config`, `set_config`, and `get_fitted_params` (only fittable estimators)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3.2 searching for regressors and metrics <a class=\"anchor\" id=\"section1_3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "as first-class citizens, all objects in `skpro` are indexed via the `registry` utility `all_objects`.\n",
    "\n",
    "To find probabilistic supervised regressors, use `all_objects` with the type `regressor_proba`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "all_objects(\"regressor_proba\", as_dataframe=True).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a full list can also be found in the online API reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for metrics, as seen above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "all_objects(\"metric\", as_dataframe=True, return_tags=\"scitype:y_pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all tags can be printed by the `all_tags` utility:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tags applicable to metrics\n",
    "from skpro.registry import all_tags\n",
    "\n",
    "all_tags(\"metric\", as_dataframe=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all tags applicable to probabilistic regressors\n",
    "from skpro.registry import all_tags\n",
    "\n",
    "all_tags(\"regressor_proba\", as_dataframe=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "filtering in search can be done with the `filter_tags` argument in `all_objects`, see docstring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.registry import all_objects\n",
    "\n",
    "# \"retrieve all genuinely probabilistic loss functions\"\n",
    "all_objects(\"metric\", as_dataframe=True, filter_tags={\"scitype:y_pred\": \"pred_proba\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced composition patterns <a class=\"anchor\" id=\"chapter3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we introduce a number of composition patterns available in `skpro`:\n",
    "\n",
    "* reducer-wrappers that turn `sklearn` regressors into probabilistic ones\n",
    "* pipelines of `sklearn` transformers with `skpro` regressors\n",
    "* tuning `skpro` probabilistic regressors via grid/random search, minimizing a probabilistic metric\n",
    "* ensembling multiple `skpro` probabilistic regressors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data used in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluation metric used in this section:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crps = CRPS()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Reducers to turn `sklearn` regressors probabilistic <a class=\"anchor\" id=\"section3_1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "there are many common algorithms that turn a non-probabilistic tabular regressor probabilistic\n",
    "\n",
    "formally, this is a type of \"reduction\" - of probabilistic supervised tabular to non-probabilistic supervised tabular\n",
    "\n",
    "Examples:\n",
    "\n",
    "* predicting variance equal to training residual variance - `ResidualDouble` with standard settings\n",
    "    * or other unconditional distribution estimate for residuals\n",
    "* \"squaring the residual\" two-step prediction - `ResidualDouble`\n",
    "* boostrap prediction intervals - `BootstrapRegressor`\n",
    "* conformal prediction intervals - contributions appreciated :-)\n",
    "* natural gradient boosting aka NGBoost - contributions appreciated :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.1 constant variance prediction <a class=\"anchor\" id=\"section3_1_1\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# estimator specification - use any sklearn regressor for reg_mean\n",
    "reg_mean = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean, cv=KFold(5))\n",
    "# cv is used to estimate out-of-sample residual variance via 5-fold CV\n",
    "# note - in-sample predictions will usually underestimate the variance!\n",
    "\n",
    "# fit and predict\n",
    "reg_proba.fit(X_train, y_train)\n",
    "y_pred_proba = reg_proba.predict_proba(X_test)\n",
    "\n",
    "# evaluate\n",
    "crps(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_interval\n",
    "\n",
    "plot_crossplot_interval(y_test, y_pred_proba, coverage=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.2 two-step residual prediction <a class=\"anchor\" id=\"section3_1_2\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# estimator specification - use any sklearn regressor for reg_mean and reg_resid\n",
    "reg_mean = RandomForestRegressor()\n",
    "reg_resid = RandomForestRegressor()\n",
    "reg_proba = ResidualDouble(reg_mean, estimator_resid=reg_resid, cv=KFold(5))\n",
    "# cv is used to estimate out-of-sample residual variance via 5-fold CV\n",
    "\n",
    "# fit and predict\n",
    "reg_proba.fit(X_train, y_train)\n",
    "y_pred_proba = reg_proba.predict_proba(X_test)\n",
    "\n",
    "# evaluate\n",
    "crps(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_interval\n",
    "\n",
    "plot_crossplot_interval(y_test, y_pred_proba, coverage=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1.3 bootstrap prediction intervals <a class=\"anchor\" id=\"section3_1_3\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from skpro.regression.bootstrap import BootstrapRegressor\n",
    "\n",
    "# estimator specification - use any sklearn regressor for reg_mean\n",
    "reg_mean = LinearRegression()\n",
    "reg_proba = BootstrapRegressor(reg_mean, n_bootstrap_samples=100)\n",
    "\n",
    "# fit and predict\n",
    "reg_proba.fit(X_train, y_train)\n",
    "y_pred_proba = reg_proba.predict_proba(X_test)\n",
    "\n",
    "# evaluate\n",
    "crps(y_test, y_pred_proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.utils.plotting import plot_crossplot_interval\n",
    "\n",
    "plot_crossplot_interval(y_test, y_pred_proba, coverage=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Pipelines of `skpro` regressor and `sklearn` transformers <a class=\"anchor\" id=\"section3_2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` regressors can be pipelined with `sklearn` transformers, using the `skpro` pipeline.\n",
    "\n",
    "This ensure presence of `predict_proba` etc in the pipeline object.\n",
    "\n",
    "The syntax is exactly the same as for `sklearn`'s pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer as Imputer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from skpro.regression.compose import Pipeline\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "# estimator specification\n",
    "reg_mean = LinearRegression()\n",
    "reg_proba = ResidualDouble(reg_mean)\n",
    "\n",
    "# pipeline is specified as a list of tuples (name, estimator)\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        (\"imputer\", Imputer()),  # an sklearn transformer\n",
    "        (\"scaler\", MinMaxScaler()),  # an sklearn transformer\n",
    "        (\"regressor\", reg_proba),  # an skpro regressor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the pipeline behaves as any skpro regressor\n",
    "pipe.fit(X_train, y_train)\n",
    "y_pred = pipe.predict(X=X_test)\n",
    "y_pred_proba = pipe.predict_proba(X=X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the pipeline provides the familiar nested `get_params`, `set_params` interface:\n",
    "\n",
    "nested parameters are keyed `componentname__parametername`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipelines can also be created via simple lists of estimators,\n",
    "\n",
    "in this case names are generated automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline is specified as a list of tuples (name, estimator)\n",
    "pipe = Pipeline(\n",
    "    steps=[\n",
    "        Imputer(),  # an sklearn transformer\n",
    "        MinMaxScaler(),  # an sklearn transformer\n",
    "        reg_proba,  # an skpro regressor\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Tuning of `skpro` regressors via grid and random search <a class=\"anchor\" id=\"section3_3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`skpro` provides grid and random search tuners to tune arbitrary probabilistic regressors,\n",
    "\n",
    "using probabilistic metrics. Besides this, they function as the `sklearn` tuners do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X, y = load_diabetes(return_X_y=True, as_frame=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "from skpro.metrics import CRPS\n",
    "from skpro.model_selection import GridSearchCV\n",
    "from skpro.regression.residual import ResidualDouble\n",
    "\n",
    "# cross-validation specification for tuner\n",
    "cv = KFold(n_splits=3)\n",
    "\n",
    "# estimator to be tuned\n",
    "estimator = ResidualDouble(LinearRegression())\n",
    "\n",
    "# tuning grid - do we fit an intercept in the linear regression?\n",
    "param_grid = {\"estimator__fit_intercept\" : [True, False]}\n",
    "\n",
    "# metric to be optimized\n",
    "crps_metric = CRPS()\n",
    "\n",
    "# specification of the grid search tuner\n",
    "gscv = GridSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring=crps_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the grid search tuner behaves like any `skpro` probabilistic regressor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gscv.fit(X_train, y_train)\n",
    "y_pred = gscv.predict(X_test)\n",
    "y_pred_proba = gscv.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "random search is similar, except that instead of a grid a parameter sampler should be specified:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skpro.model_selection import RandomizedSearchCV\n",
    "\n",
    "# only difference to GridSearchCV is the param_distributions argument\n",
    "\n",
    "# specification of the random search parameter sampler\n",
    "param_distributions = {\"estimator__fit_intercept\" : [True, False]}\n",
    "\n",
    "# specification of the random search tuner\n",
    "rscv = RandomizedSearchCV(\n",
    "    estimator=estimator,\n",
    "    param_distributions=param_distributions,\n",
    "    cv=cv,\n",
    "    scoring=crps_metric,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "skpro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3e631b8a076cc106144e9b132b7d31cae2f1e2660b47e5f9fcb0397caae5fbd5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
